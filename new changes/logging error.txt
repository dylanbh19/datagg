import logging
import json
import os
import sys # Import sys for StreamHandler
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional
from enum import Enum # Assuming LogLevel is an Enum

# Ensure LogLevel Enum is defined as it's used in EnterpriseLogger
class LogLevel(Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

class EnterpriseLogger:
    """Production-grade logging system with comprehensive monitoring"""
    
    def __init__(self, config: Dict):
        # Retrieve configuration values
        self.name = config.get("LOG_NAME", "CustomerCommsDashboard")
        self.level_enum = config.get("LOG_LEVEL", LogLevel.INFO)
        self.log_file_path = config.get("LOG_FILE_PATH", f"logs/{self.name}_{datetime.now().strftime('%Y%m%d')}.log")

        self.logger = logging.getLogger(self.name)
        # Set logger level using the logging module's constants
        self.logger.setLevel(getattr(logging, self.level_enum.value))
        
        # Initialize counters for errors and warnings
        self.error_count = 0
        self.warning_count = 0

        # Clear existing handlers to prevent duplicate log messages
        if self.logger.handlers:
            for handler in list(self.logger.handlers):
                self.logger.removeHandler(handler)
        
        # Create formatters
        detailed_formatter = logging.Formatter(
            '%(asctime)s | %(name)s | %(levelname)s | %(funcName)s:%(lineno)d | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        console_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)s | %(message)s',
            datefmt='%H:%M:%S'
        )
        
        # Console handler (outputs to stdout, typically the terminal)
        # Set encoding to 'utf-8' for console output to handle emojis
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(getattr(logging, self.level_enum.value)) # Set handler level
        console_handler.setFormatter(console_formatter)
        self.logger.addHandler(console_handler)
        
        # File handler for persistence
        try:
            log_dir = Path(self.log_file_path).parent
            log_dir.mkdir(parents=True, exist_ok=True) # Ensure log directory exists
            
            # Use 'utf-8' encoding for the file handler to prevent UnicodeEncodeError
            file_handler = logging.FileHandler(
                self.log_file_path,
                encoding='utf-8'
            )
            file_handler.setLevel(logging.DEBUG) # File handler typically logs all debug messages
            file_handler.setFormatter(detailed_formatter)
            self.logger.addHandler(file_handler)
            
        except Exception as e:
            # Log this warning through the console handler if file handler fails
            self.logger.warning(f"Could not create file handler: {e}")
        
        self.logger.info(f"üöÄ Logger initialized. Log file: {os.path.abspath(self.log_file_path)}")
    
    def debug(self, message: str, extra_data: Optional[Dict] = None):
        """Debug level logging with optional structured data"""
        if extra_data:
            try:
                self.logger.debug(f"{message} | Data: {json.dumps(extra_data, default=str)}")
            except TypeError:
                self.logger.debug(f"{message} | Data: {str(extra_data)}")
        else:
            self.logger.debug(message)
    
    def info(self, message: str, extra_data: Optional[Dict] = None):
        """Info level logging with optional structured data"""
        if extra_data:
            try:
                self.logger.info(f"{message} | Data: {json.dumps(extra_data, default=str)}")
            except TypeError:
                self.logger.info(f"{message} | Data: {str(extra_data)}")
        else:
            self.logger.info(message)
    
    def warning(self, message: str, extra_data: Optional[Dict] = None):
        """Warning level logging with tracking"""
        self.warning_count += 1
        if extra_data:
            try:
                self.logger.warning(f"{message} | Data: {json.dumps(extra_data, default=str)}")
            except TypeError:
                self.logger.warning(f"{message} | Data: {str(extra_data)}")
        else:
            self.logger.warning(message)
    
    def error(self, message: str, exception: Optional[Exception] = None, extra_data: Optional[Dict] = None):
        """Error level logging with exception tracking"""
        self.error_count += 1
        error_msg = message
        
        if exception:
            error_msg += f" | Exception: {str(exception)}"
            # traceback.format_exc() is typically used when you want to get the traceback string
            # and log it explicitly. logging.exception() automatically includes traceback.
            # For this structured logging, we'll just include the exception string.
            # If you want full traceback with every error, use logging.exception(error_msg)
            # or add traceback.format_exc() conditionally.
            # For now, let's keep it clean as per your original error method structure.
            # If ENTERPRISE_CONFIG['DEBUG_MODE'] is true, you could add traceback explicitly.
            
        if extra_data:
            try:
                error_msg += f" | Data: {json.dumps(extra_data, default=str)}"
            except TypeError:
                error_msg += f" | Data: {str(extra_data)}"
        
        self.logger.error(error_msg, exc_info=exception is not None) # exc_info=True logs the current exception traceback
    
    def critical(self, message: str, exception: Optional[Exception] = None, extra_data: Optional[Dict] = None):
        """Critical level logging for system failures"""
        self.error_count += 1 # Critical errors also count as errors
        critical_msg = message
        
        if exception:
            critical_msg += f" | Exception: {str(exception)}"
            # Similar to error, logging.critical with exc_info=True will include traceback
            
        if extra_data:
            try:
                critical_msg += f" | Data: {json.dumps(extra_data, default=str)}"
            except TypeError:
                critical_msg += f" | Data: {str(extra_data)}"
        
        self.logger.critical(critical_msg, exc_info=exception is not None)
    
    def get_stats(self) -> Dict:
        """Get logging statistics"""
        return {
            'error_count': self.error_count,
            'warning_count': self.warning_count,
            'log_level': self.level_enum.value
        }













class EnterpriseLogger:
    def __init__(self, config: Dict):
        self.log_file = config["LOG_FILE_PATH"]
        self.logger = logging.getLogger(config["LOG_NAME"])
        # Set log level dynamically from config
        self.logger.setLevel(getattr(logging, config.get('LOG_LEVEL', LogLevel.INFO).name))

        # Create handlers
        c_handler = logging.StreamHandler()
        f_handler = logging.FileHandler(self.log_file, encoding='utf-8') # Add encoding here

        # Create formatters and add it to handlers
        c_format = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        f_format = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        c_handler.setFormatter(c_format)
        f_handler.setFormatter(f_format)

        # Add handlers to the logger
        self.logger.addHandler(c_handler)
        self.logger.addHandler(f_handler)

        self.logger.info(f"üöÄ Logger initialized. Log file: {os.path.abspath(self.log_file)}")





AttributeError: 'EnterpriseLogger' object has no attribute 'info'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BhungarD\OneDrive - Computershare\Desktop\prod - mail call analysis and intent\eda.py", line 2959, in <module>
    success = main()
  File "c:\Users\BhungarD\OneDrive - Computershare\Desktop\prod - mail call analysis and intent\eda.py", line 2908, in main
    logger.critical("‚ùå Critical system failure", e)
    ^^^^^^^^^^^^^^^
AttributeError: 'EnterpriseLogger' object has no attribute 'critical'



To fix the `TypeError` in your `logging error.txt` file, you need to adjust how the `EnterpriseLogger` class is initialized and how its `__init__` method uses the provided configuration.

[cite\_start]The error "TypeError: EnterpriseLogger.**init**() takes 2 positional arguments but 3 were given" [cite: 1] indicates that when you call `EnterpriseLogger`, you are passing too many arguments.

Here's a breakdown of the problem and the solution:

**Problem:**

  * [cite\_start]Your `EnterpriseLogger` class's `__init__` method is defined to accept `self` and one `config` dictionary argument: `def __init__(self, config: Dict):`[cite: 11].
  * [cite\_start]However, you are calling it with two explicit arguments: `logger = EnterpriseLogger("CustomerCommsDashboard", ENTERPRISE_CONFIG['LOG_LEVEL'])`[cite: 1]. The `self` argument is implicit, making it effectively three arguments being passed.

**Solution:**

To resolve this, you should modify your `ENTERPRISE_CONFIG` dictionary to include the `LOG_FILE_PATH` and `LOG_NAME`, and then pass the entire `ENTERPRISE_CONFIG` dictionary to the `EnterpriseLogger`. Also, the logger's level should be set dynamically from the config.

Here are the changes you need to make in your `review.py` file based on the `logging error.txt` content:

1.  **Update `ENTERPRISE_CONFIG`**: Add `LOG_FILE_PATH` and `LOG_NAME` to your `ENTERPRISE_CONFIG` dictionary. You should place the `logs` folder in the same directory as your script.

    [cite\_start]Find this section (around [cite: 9]):

    ```python
    ENTERPRISE_CONFIG = {
        # PRIMARY DATA SOURCES
        'CALL_VOLUME_FILE': r'data\GenesysExtract_20250703.csv',           # Primary for correlations (Date column)
        'CALL_INTENTS_FILE': r'data\GenesysExtract_20250609.csv',  # Secondary for intent analysis
        'MAIL_FILE_PATH': r'merged_output.csv',                 # Mail data
    ```

    And add the log file path and name:

    ```python
    ENTERPRISE_CONFIG = {
        # LOGGING CONFIGURATION
        'LOG_FILE_PATH': r'logs\dashboard.log', # Define the log file path
        'LOG_NAME': 'CustomerCommsDashboard',  # Define the logger name
        # PRIMARY DATA SOURCES
        'CALL_VOLUME_FILE': r'data\GenesysExtract_20250703.csv',           # Primary for correlations (Date column)
        'CALL_INTENTS_FILE': r'data\GenesysExtract_20250609.csv',  # Secondary for intent analysis
        'MAIL_FILE_PATH': r'merged_output.csv',                 # Mail data
    ```

2.  **Modify `EnterpriseLogger.__init__`**: In the `EnterpriseLogger` class, adjust the `__init__` method to retrieve the log level from the `config` dictionary.

    [cite\_start]Find this section (around [cite: 11]):

    ```python
    class EnterpriseLogger:
        def __init__(self, config: Dict):
            self.log_file = config["LOG_FILE_PATH"]
            self.logger = logging.getLogger(config["LOG_NAME"])
            self.logger.setLevel(logging.INFO) # This line needs change
    ```

    Change `self.logger.setLevel(logging.INFO)` to dynamically use the `LOG_LEVEL` from your `ENTERPRISE_CONFIG`:

    ```python
    class EnterpriseLogger:
        def __init__(self, config: Dict):
            self.log_file = config["LOG_FILE_PATH"]
            self.logger = logging.getLogger(config["LOG_NAME"])
            # Set log level dynamically from config
            self.logger.setLevel(getattr(logging, config.get('LOG_LEVEL', LogLevel.INFO).name))
    ```

    *Note: `getattr(logging, config.get('LOG_LEVEL', LogLevel.INFO).name)` is used to safely get the logging level from the `LogLevel` enum and convert it to the corresponding `logging` module constant.*

3.  **Correct the `EnterpriseLogger` initialization call**: Change how `EnterpriseLogger` is instantiated in your `main` function (or wherever it's called).

    [cite\_start]Find this line (around [cite: 1]):

    ```python
    logger = EnterpriseLogger("CustomerCommsDashboard", ENTERPRISE_CONFIG['LOG_LEVEL'])
    ```

    Replace it with:

    ```python
    logger = EnterpriseLogger(ENTERPRISE_CONFIG)
    ```

By making these changes, the `EnterpriseLogger` will be initialized correctly with a single configuration dictionary, resolving the `TypeError`.

This fix ensures that the logging system is properly configured according to the `ENTERPRISE_CONFIG` and aligns with the `EnterpriseLogger` class's expected arguments.































Traceback (most recent call last):
  File "c:\Users\BhungarD\OneDrive - Computershare\Desktop\prod - mail call analysis and intent\eda.py", line 2956, in <module>
    success = main()
  File "c:\Users\BhungarD\OneDrive - Computershare\Desktop\prod - mail call analysis and intent\eda.py", line 2762, in main
    logger = EnterpriseLogger("CustomerCommsDashboard", ENTERPRISE_CONFIG['LOG_LEVEL'])
TypeError: EnterpriseLogger.__init__() takes 2 positional arguments but 3 were given



#!/usr/bin/env python3
"""
Customer Communications Intelligence Dashboard v3.0 - PRODUCTION GRADE
Enterprise-Level Analytics Platform with Advanced Visualizations

PRODUCTION ENHANCEMENTS:
- Dual call data source management (volumes + intents)
- Mail data filtered to only include days with call data
- Ultra-sleek, high-end visual design with premium styling
- Comprehensive error handling with graceful fallbacks
- Production-grade logging and monitoring
- Advanced correlation analysis with 4 new analytical plots
- Optimized data processing with robust validation

Data Strategy:
- Call Volumes File: Primary for correlations, overlays (Date column)
- Call Intents File: Secondary for intent analysis only (ConversationStart, uui_Intent)
- Mail Data: Filtered to only include dates with call data presence
"""

import pandas as pd
import numpy as np
import warnings
import os
import sys
from datetime import datetime, timedelta
import logging
from pathlib import Path
import holidays
from typing import Dict, List, Optional, Tuple, Any
import json
from dataclasses import dataclass
from enum import Enum
import traceback

# Suppress warnings for production
warnings.filterwarnings('ignore')
pd.options.mode.chained_assignment = None
np.seterr(divide='ignore', invalid='ignore')

# Core visualization libraries
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    import plotly.figure_factory as ff
    PLOTLY_AVAILABLE = True
except ImportError as e:
    print(f"CRITICAL: Plotly unavailable - {e}")
    PLOTLY_AVAILABLE = False

# Dashboard framework
try:
    import dash
    from dash import dcc, html, Input, Output, dash_table, State, callback_context
    import dash_bootstrap_components as dbc
    from dash.exceptions import PreventUpdate
    DASH_AVAILABLE = True
except ImportError as e:
    print(f"CRITICAL: Dash unavailable - {e}")
    DASH_AVAILABLE = False

# Statistical libraries
try:
    from scipy import stats
    from scipy.stats import pearsonr, spearmanr
    from scipy.signal import savgol_filter
    from scipy.stats import zscore
    SCIPY_AVAILABLE = True
except ImportError as e:
    print(f"WARNING: SciPy unavailable - advanced analytics disabled - {e}")
    SCIPY_AVAILABLE = False

# Machine learning libraries
try:
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.decomposition import PCA
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import r2_score
    from sklearn.ensemble import IsolationForest
    SKLEARN_AVAILABLE = True
except ImportError as e:
    print(f"WARNING: Scikit-learn unavailable - ML features disabled - {e}")
    SKLEARN_AVAILABLE = False

# Time series analysis
try:
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.tsa.stattools import adfuller
    STATSMODELS_AVAILABLE = True
except ImportError as e:
    print(f"WARNING: Statsmodels unavailable - time series analysis limited - {e}")
    STATSMODELS_AVAILABLE = False

# Financial data
try:
    import yfinance as yf
    FINANCIAL_AVAILABLE = True
except ImportError as e:
    print(f"INFO: yfinance unavailable - financial indicators disabled - {e}")
    FINANCIAL_AVAILABLE = False

# =============================================================================
# ENTERPRISE CONFIGURATION & VISUAL THEME
# =============================================================================

class LogLevel(Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

@dataclass
class PremiumVisualTheme:
    """Ultra-premium visual theme for executive dashboards"""
    
    # Executive color palette - sophisticated and professional
    primary_navy = '#0F172A'        # Deep navy for headers
    primary_blue = '#1E40AF'        # Professional blue
    primary_teal = '#0891B2'        # Modern teal
    primary_emerald = '#059669'     # Success green
    primary_amber = '#D97706'       # Warning amber
    primary_rose = '#E11D48'        # Alert rose
    primary_violet = '#7C3AED'      # Premium violet
    primary_indigo = '#4F46E5'      # Rich indigo
    
    # Accent colors for highlights
    accent_gold = '#F59E0B'         # Premium gold
    accent_cyan = '#06B6D4'         # Modern cyan
    accent_lime = '#65A30D'         # Fresh lime
    accent_fuchsia = '#C026D3'      # Bold fuchsia
    
    # Neutral palette - for backgrounds and text
    neutral_50 = '#F8FAFC'          # Lightest background
    neutral_100 = '#F1F5F9'         # Light background
    neutral_200 = '#E2E8F0'         # Border light
    neutral_300 = '#CBD5E1'         # Border medium
    neutral_400 = '#94A3B8'         # Text light
    neutral_500 = '#64748B'         # Text medium
    neutral_600 = '#475569'         # Text dark
    neutral_700 = '#334155'         # Heading dark
    neutral_800 = '#1E293B'         # Heading darker
    neutral_900 = '#0F172A'         # Darkest text
    
    # Semantic colors
    success = '#10B981'             # Success operations
    warning = '#F59E0B'             # Warning alerts  
    error = '#EF4444'               # Error states
    info = '#3B82F6'                # Information
    
    # Premium gradients
    gradient_primary = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)'
    gradient_success = 'linear-gradient(135deg, #11998e 0%, #38ef7d 100%)'
    gradient_warning = 'linear-gradient(135deg, #f093fb 0%, #f5576c 100%)'
    gradient_dark = 'linear-gradient(135deg, #2c3e50 0%, #3498db 100%)'
    
    # Typography - premium fonts
    font_family = "'Inter', 'SF Pro Display', 'Segoe UI', 'Roboto', sans-serif"
    font_mono = "'JetBrains Mono', 'SF Mono', 'Consolas', monospace"
    font_display = "'Poppins', 'Inter', sans-serif"
    
    # Chart-specific colors
    chart_colors = [
        '#1E40AF',  # Primary blue
        '#DC2626',  # Red
        '#059669',  # Green  
        '#7C3AED',  # Purple
        '#EA580C',  # Orange
        '#0891B2',  # Teal
        '#BE185D',  # Pink
        '#4338CA',  # Indigo
        '#0D9488',  # Emerald
        '#C2410C'   # Orange-red
    ]
    
    # Correlation heatmap colors
    correlation_colorscale = [
        [0.0, '#DC2626'],    # Strong negative - red
        [0.25, '#F87171'],   # Weak negative - light red
        [0.5, '#F3F4F6'],    # Neutral - light gray
        [0.75, '#60A5FA'],   # Weak positive - light blue
        [1.0, '#1E40AF']     # Strong positive - blue
    ]

THEME = PremiumVisualTheme()

# Enterprise configuration with dual call data strategy
ENTERPRISE_CONFIG = {
    # PRIMARY DATA SOURCES
    'CALL_VOLUME_FILE': r'data\GenesysExtract_20250703.csv',           # Primary for correlations (Date column)
    'CALL_INTENTS_FILE': r'data\GenesysExtract_20250609.csv',  # Secondary for intent analysis
    'MAIL_FILE_PATH': r'merged_output.csv',                 # Mail data
    
    # COLUMN MAPPINGS
    'MAIL_COLUMNS': {'date': 'mail_date', 'volume': 'mail_volume', 'type': 'mail_type'},
    'CALL_VOLUME_COLUMNS': {'date': 'Date'},  # Call volumes file
    'CALL_INTENT_COLUMNS': {'date': 'ConversationStart', 'intent': 'uui_Intent'},  # Intent file
    
    # FINANCIAL DATA
    'FINANCIAL_INDICATORS': {
        'S&P 500': '^GSPC',
        '10-Year Treasury': '^TNX',
        'VIX': '^VIX',
        'Dollar Index': 'DX-Y.NYB',
        'Gold': 'GC=F',
        'Oil': 'CL=F'
    },
    
    # ANALYSIS PARAMETERS
    'MAX_LAG_DAYS': 21,
    'CORRELATION_WINDOWS': [7, 14, 30],
    'MOMENTUM_WINDOWS': [3, 7, 14],
    'SMOOTHING_WINDOWS': [3, 5, 7],
    'MIN_CORRELATION_THRESHOLD': 0.15,
    'SIGNIFICANCE_LEVEL': 0.05,
    'ANOMALY_THRESHOLD': 2.5,
    'SPIKE_THRESHOLD_PERCENTILE': 90,
    'ROLLING_WINDOW_SIZE': 30,
    
    # DATA QUALITY FILTERS
    'FILTER_WEEKENDS': True,
    'FILTER_HOLIDAYS': True,
    'USE_INNER_JOIN': True,        # Only keep dates with both mail and call data
    'MIN_SAMPLE_SIZE': 20,
    'OUTLIER_PERCENTILE': 95,
    'DATA_START_DATE': '2024-06-01',
    'MIN_MAIL_VOLUME': 10,         # Minimum daily mail volume to include
    
    # VISUAL SETTINGS
    'CHART_HEIGHT': 600,
    'CHART_HEIGHT_LARGE': 800,
    'CHART_HEIGHT_SMALL': 400,
    'ANIMATION_DURATION': 750,
    'HOVER_DISTANCE': 100,
    'TOP_N_DISPLAY': 10,
    'CHART_MARGIN': {'t': 100, 'b': 60, 'l': 80, 'r': 80},
    
    # PERFORMANCE SETTINGS
    'MAX_RECORDS_DISPLAY': 10000,
    'CACHE_TIMEOUT': 300,
    'ASYNC_PROCESSING': True,
    'PARALLEL_WORKERS': 4,
    
    # PRODUCTION SETTINGS
    'DEBUG_MODE': False,
    'ENABLE_PROFILING': False,
    'LOG_LEVEL': LogLevel.INFO,
    'BACKUP_RETENTION_DAYS': 30,
    'MONITORING_ENABLED': True,
    'ERROR_THRESHOLD': 5,          # Max errors before graceful degradation
    'PERFORMANCE_MONITORING': True
}

# =============================================================================
# ENTERPRISE LOGGING SYSTEM
# =============================================================================

import logging
import os

class EnterpriseLogger:
    def __init__(self, config: Dict):
        self.log_file = config["LOG_FILE_PATH"]
        self.logger = logging.getLogger(config["LOG_NAME"])
        self.logger.setLevel(logging.INFO)

        # Create handlers
        c_handler = logging.StreamHandler()
        f_handler = logging.FileHandler(self.log_file, encoding='utf-8') # Add encoding here

        # Create formatters and add it to handlers
        c_format = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        f_format = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        c_handler.setFormatter(c_format)
        f_handler.setFormatter(f_format)

        # Add handlers to the logger
        self.logger.addHandler(c_handler)
        self.logger.addHandler(f_handler)

        self.logger.info(f"üöÄ Logger initialized. Log file: {os.path.abspath(self.log_file)}")

# =============================================================================
# ENTERPRISE DATA PROCESSOR v3.0
# =============================================================================

class EnterpriseDataProcessor:
    """
    Production-grade data processor with dual call file management
    
    Strategy:
    - Call Volume File: Primary source for correlations and overlays
    - Call Intent File: Secondary source for intent analysis only
    - Mail Data: Filtered to only include dates with call data
    """
    
    def __init__(self, config: Dict, logger: EnterpriseLogger):
        self.config = config
        self.logger = logger
        
        # Core datasets
        self.mail_df = pd.DataFrame()
        self.call_volume_df = pd.DataFrame()      # Primary call data
        self.call_intent_df = pd.DataFrame()      # Secondary call data
        self.financial_df = pd.DataFrame()
        self.combined_df = pd.DataFrame()
        
        # Data availability flags
        self.has_call_volume_data = False
        self.has_call_intent_data = False
        self.has_mail_data = False
        self.has_financial_data = False
        
        # Analysis results
        self.momentum_correlation_results = pd.DataFrame()
        self.rolling_correlation_results = pd.DataFrame()
        self.intent_correlation_results = pd.DataFrame()
        self.correlation_data_table = pd.DataFrame()
        self.mail_by_type = pd.DataFrame()
        self.calls_by_intent = pd.DataFrame()
        self.efficiency_metrics = {}
        self.anomaly_data = pd.DataFrame()
        self.decomposition_results = {}
        
        # Data quality metrics
        self.data_quality_metrics = {
            'total_records_processed': 0,
            'records_after_filtering': 0,
            'weekends_removed': 0,
            'holidays_removed': 0,
            'outliers_removed': 0,
            'missing_data_imputed': 0
        }
        
        # Initialize US holidays
        self.us_holidays = holidays.UnitedStates()
        
        self.logger.info("Enterprise Data Processor v3.0 initialized")
    
    def safe_load_csv(self, file_path: str, description: str = "file") -> pd.DataFrame:
        """
        Production-grade CSV loading with comprehensive error handling
        """
        try:
            if not file_path or not os.path.exists(file_path):
                self.logger.warning(f"{description} not found: {file_path}")
                return pd.DataFrame()
            
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                self.logger.warning(f"{description} is empty: {file_path}")
                return pd.DataFrame()
            
            # Try multiple encodings for robust loading
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(
                        file_path, 
                        encoding=encoding, 
                        on_bad_lines='skip',
                        low_memory=False,
                        dtype=str,  # Load as string first to avoid data type issues
                        na_values=['', 'NULL', 'null', 'NA', 'N/A', 'nan', 'NaN']
                    )
                    
                    if not df.empty:
                        self.logger.info(f"‚úÖ {description} loaded successfully", {
                            'file_path': file_path,
                            'records': len(df),
                            'columns': len(df.columns),
                            'encoding': encoding,
                            'file_size_mb': round(file_size / (1024 * 1024), 2)
                        })
                        return df
                    
                except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError) as e:
                    self.logger.debug(f"Encoding {encoding} failed for {description}: {e}")
                    continue
                except Exception as e:
                    self.logger.error(f"Unexpected error loading {description} with {encoding}", e)
                    continue
            
            self.logger.error(f"‚ùå Failed to load {description} with any encoding")
            return pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"‚ùå Critical error loading {description}", e)
            return pd.DataFrame()
    
    def safe_date_conversion(self, df: pd.DataFrame, date_column: str, 
                           data_type: str = "data") -> pd.DataFrame:
        """
        Production-grade date conversion with comprehensive validation
        """
        try:
            if df.empty or date_column not in df.columns:
                self.logger.warning(f"Date column '{date_column}' not found in {data_type}")
                return df
            
            original_count = len(df)
            
            # Convert to datetime with multiple format attempts
            date_formats = [
                '%Y-%m-%d',
                '%m/%d/%Y',
                '%d/%m/%Y',
                '%Y-%m-%d %H:%M:%S',
                '%m/%d/%Y %H:%M:%S',
                None  # Let pandas infer
            ]
            
            successful_conversion = False
            for fmt in date_formats:
                try:
                    if fmt:
                        df[date_column] = pd.to_datetime(df[date_column], format=fmt, errors='coerce')
                    else:
                        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
                    
                    # Check if conversion was successful
                    valid_dates = df[date_column].notna().sum()
                    if valid_dates > 0:
                        successful_conversion = True
                        break
                        
                except Exception as e:
                    self.logger.debug(f"Date format {fmt} failed: {e}")
                    continue
            
            if not successful_conversion:
                self.logger.error(f"‚ùå Could not convert {date_column} to datetime in {data_type}")
                return df
            
            # Remove rows with invalid dates
            df = df.dropna(subset=[date_column])
            
            # Normalize dates to remove time component
            df[date_column] = df[date_column].dt.tz_localize(None).dt.normalize()
            
            # Validate date range
            min_date = df[date_column].min()
            max_date = df[date_column].max()
            
            if min_date < pd.Timestamp('2020-01-01') or max_date > pd.Timestamp('2030-12-31'):
                self.logger.warning(f"Unusual date range in {data_type}: {min_date} to {max_date}")
            
            success_rate = len(df) / original_count * 100
            self.logger.info(f"Date conversion completed for {data_type}", {
                'success_rate': f"{success_rate:.1f}%",
                'valid_records': len(df),
                'original_records': original_count,
                'date_range': f"{min_date.date()} to {max_date.date()}"
            })
            
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Date conversion failed for {data_type}", e)
            return df
    def load_call_volume_data(self) -> bool:
            """
            Load primary call volume data (Date column) - used for correlations and overlays
            """
            self.logger.info("üîÑ Loading primary call volume data...")
            
            try:
                # Load call volume file
                self.call_volume_df = self.safe_load_csv(
                    self.config['CALL_VOLUME_FILE'], 
                    "Call Volume Data"
                )
                
                if self.call_volume_df.empty:
                    self.logger.warning("‚ùå No call volume data loaded")
                    return False
                
                # Map columns
                date_col = self.config['CALL_VOLUME_COLUMNS']['date']
                volume_col = self.config['CALL_VOLUME_COLUMNS'].get('volume', None)
                
                # Validate required columns exist
                if date_col not in self.call_volume_df.columns:
                    self.logger.error(f"Date column '{date_col}' not found in call volume data")
                    return False
                
                # Convert date column
                self.call_volume_df = self.safe_date_conversion(
                    self.call_volume_df, date_col, "call volume"
                )
                
                if self.call_volume_df.empty:
                    self.logger.error("‚ùå No valid dates in call volume data")
                    return False
                
                # Rename columns for consistency
                self.call_volume_df = self.call_volume_df.rename(columns={
                    date_col: 'date'
                })
                
                # Handle call volume column
                if volume_col in self.call_volume_df.columns:
                    # If explicit volume column exists, use it
                    self.call_volume_df = self.call_volume_df.rename(columns={
                        volume_col: 'call_volume'
                    })
                else:
                    # If no volume column, count daily occurrences
                    self.call_volume_df = (self.call_volume_df
                                        .groupby('date')
                                        .size()
                                        .reset_index(name='call_volume'))
                
                # Convert volume to numeric and validate
                self.call_volume_df['call_volume'] = pd.to_numeric(
                    self.call_volume_df['call_volume'], errors='coerce'
                )
                self.call_volume_df = self.call_volume_df.dropna(subset=['call_volume'])
                
                # Filter by date range
                start_date = pd.to_datetime(self.config['DATA_START_DATE'])
                original_len = len(self.call_volume_df)
                self.call_volume_df = self.call_volume_df[
                    self.call_volume_df['date'] >= start_date
                ]
                
                # Remove weekends and holidays if configured
                if self.config['FILTER_WEEKENDS']:
                    weekend_count = len(self.call_volume_df[self.call_volume_df['date'].dt.weekday >= 5])
                    self.call_volume_df = self.call_volume_df[
                        self.call_volume_df['date'].dt.weekday < 5
                    ]
                    self.data_quality_metrics['weekends_removed'] += weekend_count
                
                if self.config['FILTER_HOLIDAYS']:
                    holiday_mask = self.call_volume_df['date'].dt.date.isin(self.us_holidays)
                    holiday_count = holiday_mask.sum()
                    self.call_volume_df = self.call_volume_df[~holiday_mask]
                    self.data_quality_metrics['holidays_removed'] += holiday_count
                
                # Final validation
                if len(self.call_volume_df) < self.config['MIN_SAMPLE_SIZE']:
                    self.logger.warning(f"Insufficient call volume data: {len(self.call_volume_df)} records")
                    return False
                
                self.has_call_volume_data = True
                self.logger.info("‚úÖ Call volume data loaded successfully", {
                    'total_records': len(self.call_volume_df),
                    'date_range': f"{self.call_volume_df['date'].min().date()} to {self.call_volume_df['date'].max().date()}",
                    'avg_daily_volume': f"{self.call_volume_df['call_volume'].mean():.1f}",
                    'filtered_out': original_len - len(self.call_volume_df)
                })
                
                return True
                
            except Exception as e:
                self.logger.error("‚ùå Failed to load call volume data", e)
                return False
        
    def load_call_intent_data(self) -> bool:
        """
        Load secondary call intent data (ConversationStart, uui_Intent) - used for intent analysis only
        """
        self.logger.info("üîÑ Loading secondary call intent data...")
        
        try:
            # Load call intent file
            self.call_intent_df = self.safe_load_csv(
                self.config['CALL_INTENTS_FILE'], 
                "Call Intent Data"
            )
            
            if self.call_intent_df.empty:
                self.logger.warning("‚ùå No call intent data loaded")
                return False
            
            # Map columns
            date_col = self.config['CALL_INTENT_COLUMNS']['date']
            intent_col = self.config['CALL_INTENT_COLUMNS']['intent']
            
            # Validate required columns exist
            required_cols = [date_col, intent_col]
            missing_cols = [col for col in required_cols if col not in self.call_intent_df.columns]
            
            if missing_cols:
                self.logger.error(f"Missing columns in intent data: {missing_cols}")
                return False
            
            # Convert date column
            self.call_intent_df = self.safe_date_conversion(
                self.call_intent_df, date_col, "call intent"
            )
            
            if self.call_intent_df.empty:
                self.logger.error("‚ùå No valid dates in call intent data")
                return False
            
            # Rename columns for consistency
            self.call_intent_df = self.call_intent_df.rename(columns={
                date_col: 'date',
                intent_col: 'intent'
            })
            
            # Clean intent data
            self.call_intent_df['intent'] = self.call_intent_df['intent'].astype(str).str.strip()
            self.call_intent_df = self.call_intent_df[
                (self.call_intent_df['intent'] != '') & 
                (self.call_intent_df['intent'].notna()) &
                (self.call_intent_df['intent'] != 'nan')
            ]
            
            # Filter by date range
            start_date = pd.to_datetime(self.config['DATA_START_DATE'])
            original_len = len(self.call_intent_df)
            self.call_intent_df = self.call_intent_df[
                self.call_intent_df['date'] >= start_date
            ]
            
            # Remove weekends and holidays if configured
            if self.config['FILTER_WEEKENDS']:
                self.call_intent_df = self.call_intent_df[
                    self.call_intent_df['date'].dt.weekday < 5
                ]
            
            if self.config['FILTER_HOLIDAYS']:
                holiday_mask = self.call_intent_df['date'].dt.date.isin(self.us_holidays)
                self.call_intent_df = self.call_intent_df[~holiday_mask]
            
            # Create intent summaries
            self.calls_by_intent = (self.call_intent_df
                                    .groupby('intent')
                                    .size()
                                    .reset_index(name='count')
                                    .sort_values('count', ascending=False))
            
            # Final validation
            if len(self.call_intent_df) < self.config['MIN_SAMPLE_SIZE']:
                self.logger.warning(f"Insufficient call intent data: {len(self.call_intent_df)} records")
                return False
            
            self.has_call_intent_data = True
            self.logger.info("‚úÖ Call intent data loaded successfully", {
                'total_records': len(self.call_intent_df),
                'unique_intents': len(self.calls_by_intent),
                'date_range': f"{self.call_intent_df['date'].min().date()} to {self.call_intent_df['date'].max().date()}",
                'top_intent': self.calls_by_intent.iloc[0]['intent'] if not self.calls_by_intent.empty else 'None',
                'filtered_out': original_len - len(self.call_intent_df)
            })
            
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Failed to load call intent data", e)
            return False

    def load_mail_data(self) -> bool:
        """
        Load mail data and filter to only include dates with call data
        """
        self.logger.info("üîÑ Loading mail data...")
        
        try:
            # Load mail file
            self.mail_df = self.safe_load_csv(
                self.config['MAIL_FILE_PATH'], 
                "Mail Data"
            )
            
            if self.mail_df.empty:
                self.logger.warning("‚ùå No mail data loaded")
                return False
            
            # Map columns
            date_col = self.config['MAIL_COLUMNS']['date']
            volume_col = self.config['MAIL_COLUMNS']['volume']
            type_col = self.config['MAIL_COLUMNS']['type']
            
            # Validate required columns exist
            required_cols = [date_col, volume_col]
            missing_cols = [col for col in required_cols if col not in self.mail_df.columns]
            
            if missing_cols:
                self.logger.error(f"Missing columns in mail data: {missing_cols}")
                return False
            
            # Convert date column
            self.mail_df = self.safe_date_conversion(
                self.mail_df, date_col, "mail"
            )
            
            if self.mail_df.empty:
                self.logger.error("‚ùå No valid dates in mail data")
                return False
            
            # Rename columns for consistency
            column_mapping = {
                date_col: 'date',
                volume_col: 'volume'
            }
            if type_col in self.mail_df.columns:
                column_mapping[type_col] = 'type'
            
            self.mail_df = self.mail_df.rename(columns=column_mapping)
            
            # Convert volume to numeric and validate
            self.mail_df['volume'] = pd.to_numeric(
                self.mail_df['volume'], errors='coerce'
            )
            self.mail_df = self.mail_df.dropna(subset=['volume'])
            
            # Filter by date range
            start_date = pd.to_datetime(self.config['DATA_START_DATE'])
            original_len = len(self.mail_df)
            self.mail_df = self.mail_df[
                self.mail_df['date'] >= start_date
            ]
            
            # Remove weekends and holidays if configured
            if self.config['FILTER_WEEKENDS']:
                self.mail_df = self.mail_df[
                    self.mail_df['date'].dt.weekday < 5
                ]
            
            if self.config['FILTER_HOLIDAYS']:
                holiday_mask = self.mail_df['date'].dt.date.isin(self.us_holidays)
                self.mail_df = self.mail_df[~holiday_mask]
            
            # CRITICAL: Filter mail data to only include dates with call data
            if self.has_call_volume_data:
                call_dates = set(self.call_volume_df['date'].dt.date)
                before_filter = len(self.mail_df)
                self.mail_df = self.mail_df[
                    self.mail_df['date'].dt.date.isin(call_dates)
                ]
                self.logger.info(f"Filtered mail data to call dates: {len(self.mail_df)} records (was {before_filter})")
            
            # Remove low volume days
            min_volume = self.config['MIN_MAIL_VOLUME']
            self.mail_df = self.mail_df[self.mail_df['volume'] >= min_volume]
            
            # Create mail type summaries
            if 'type' in self.mail_df.columns:
                self.mail_by_type = (self.mail_df
                                    .groupby('type')['volume']
                                    .sum()
                                    .reset_index()
                                    .sort_values('volume', ascending=False))
            else:
                self.mail_by_type = pd.DataFrame({'type': ['Unknown'], 'volume': [self.mail_df['volume'].sum()]})
            
            # Final validation
            if len(self.mail_df) < self.config['MIN_SAMPLE_SIZE']:
                self.logger.warning(f"Insufficient mail data: {len(self.mail_df)} records")
                return False
            
            self.has_mail_data = True
            self.logger.info("‚úÖ Mail data loaded successfully", {
                'total_records': len(self.mail_df),
                'unique_types': len(self.mail_by_type),
                'date_range': f"{self.mail_df['date'].min().date()} to {self.mail_df['date'].max().date()}",
                'total_volume': f"{self.mail_df['volume'].sum():,.0f}",
                'avg_daily_volume': f"{self.mail_df['volume'].mean():.1f}",
                'filtered_out': original_len - len(self.mail_df)
            })
            
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Failed to load mail data", e)
            return False

    def load_financial_data(self) -> bool:
        """
        Load financial indicator data for market correlation analysis
        """
        if not FINANCIAL_AVAILABLE:
            self.logger.info("üìä Financial data loading skipped - yfinance not available")
            return False
        
        self.logger.info("üîÑ Loading financial indicator data...")
        
        try:
            # Determine date range from existing data
            if self.has_call_volume_data:
                start_date = self.call_volume_df['date'].min() - timedelta(days=5)
                end_date = self.call_volume_df['date'].max() + timedelta(days=1)
            elif self.has_mail_data:
                start_date = self.mail_df['date'].min() - timedelta(days=5)
                end_date = self.mail_df['date'].max() + timedelta(days=1)
            else:
                start_date = pd.to_datetime(self.config['DATA_START_DATE'])
                end_date = datetime.now()
            
            self.logger.info(f"Loading financial data from {start_date.date()} to {end_date.date()}")
            
            financial_data = {}
            successful_loads = 0
            
            for name, ticker in self.config['FINANCIAL_INDICATORS'].items():
                try:
                    self.logger.info(f"Fetching {name} ({ticker})...")
                    
                    ticker_obj = yf.Ticker(ticker)
                    data = ticker_obj.history(
                        start=start_date, 
                        end=end_date,
                        interval='1d',
                        auto_adjust=True,
                        prepost=False
                    )
                    
                    if not data.empty and 'Close' in data.columns:
                        # Resample to daily and forward fill
                        close_prices = data['Close'].resample('D').last().ffill()
                        
                        if len(close_prices) > 10:  # Minimum data points
                            # Calculate percentage change from first value (normalized)
                            first_price = close_prices.iloc[0]
                            if first_price > 0:
                                pct_change = ((close_prices - first_price) / first_price * 100)
                                financial_data[name] = pct_change
                                successful_loads += 1
                                self.logger.info(f"‚úÖ {name}: {len(close_prices)} data points loaded")
                            else:
                                self.logger.warning(f"Invalid first price for {name}: {first_price}")
                    else:
                        self.logger.warning(f"No valid data for {name}")
                        
                except Exception as e:
                    self.logger.warning(f"Failed to fetch {name}: {e}")
                    continue
            
            if successful_loads > 0:
                # Create financial dataframe
                self.financial_df = pd.DataFrame(financial_data)
                self.financial_df.index.name = 'date'
                self.financial_df = self.financial_df.reset_index()
                
                # Ensure date column is datetime
                self.financial_df['date'] = pd.to_datetime(self.financial_df['date']).dt.tz_localize(None).dt.normalize()
                
                # Remove weekends if configured
                if self.config['FILTER_WEEKENDS']:
                    self.financial_df = self.financial_df[
                        self.financial_df['date'].dt.weekday < 5
                    ]
                
                # Remove holidays if configured
                if self.config['FILTER_HOLIDAYS']:
                    holiday_mask = self.financial_df['date'].dt.date.isin(self.us_holidays)
                    self.financial_df = self.financial_df[~holiday_mask]
                
                self.has_financial_data = True
                self.logger.info("‚úÖ Financial data loaded successfully", {
                    'indicators': successful_loads,
                    'total_records': len(self.financial_df),
                    'date_range': f"{self.financial_df['date'].min().date()} to {self.financial_df['date'].max().date()}",
                    'indicators_loaded': list(financial_data.keys())
                })
                
                return True
            else:
                self.logger.warning("‚ùå No financial indicators loaded successfully")
                return False
                
        except Exception as e:
            self.logger.error("‚ùå Failed to load financial data", e)
            return False

    def create_sample_data(self) -> bool:
        """
        Create realistic sample data for demonstration when real data is unavailable
        """
        self.logger.info("üîß Creating sample data for demonstration...")
        
        try:
            # Generate date range (business days only)
            start_date = pd.to_datetime(self.config['DATA_START_DATE'])
            end_date = datetime.now()
            business_days = pd.bdate_range(start=start_date, end=end_date, freq='B')
            
            np.random.seed(42)  # For reproducible results
            
            # Sample call volume data
            call_volumes = []
            for date in business_days:
                # Simulate realistic call patterns with some seasonality
                base_volume = 150 + np.sin(len(call_volumes) / 30) * 30  # Monthly cycle
                daily_volume = max(int(base_volume + np.random.normal(0, 25)), 10)
                call_volumes.append(daily_volume)
            
            self.call_volume_df = pd.DataFrame({
                'date': business_days,
                'call_volume': call_volumes
            })
            self.has_call_volume_data = True
            
            # Sample call intent data
            intents = ['billing_inquiry', 'account_update', 'statement_question', 'balance_check', 'general_support']
            intent_data = []
            
            for i, date in enumerate(business_days):
                daily_total = call_volumes[i]
                for intent in intents:
                    intent_count = max(int(daily_total * np.random.uniform(0.1, 0.3)), 1)
                    intent_data.extend([{'date': date, 'intent': intent}] * intent_count)
            
            self.call_intent_df = pd.DataFrame(intent_data)
            self.calls_by_intent = (self.call_intent_df
                                    .groupby('intent')
                                    .size()
                                    .reset_index(name='count')
                                    .sort_values('count', ascending=False))
            self.has_call_intent_data = True
            
            # Sample mail data
            mail_data = []
            mail_types = ['statement', 'billing', 'notification', 'reminder', 'confirmation']
            
            for i, date in enumerate(business_days):
                base_mail = 2000 + np.sin(i / 20) * 500  # Some seasonality
                daily_mail = max(int(base_mail + np.random.normal(0, 300)), 100)
                
                for mail_type in mail_types:
                    type_volume = max(int(daily_mail * np.random.uniform(0.1, 0.3)), 5)
                    mail_data.append({
                        'date': date,
                        'type': mail_type,
                        'volume': type_volume
                    })
            
            self.mail_df = pd.DataFrame(mail_data)
            self.mail_by_type = (self.mail_df
                                .groupby('type')['volume']
                                .sum()
                                .reset_index()
                                .sort_values('volume', ascending=False))
            self.has_mail_data = True
            
            self.logger.info("‚úÖ Sample data created successfully", {
                'business_days': len(business_days),
                'call_records': len(self.call_volume_df),
                'intent_records': len(self.call_intent_df),
                'mail_records': len(self.mail_df),
                'unique_intents': len(self.calls_by_intent),
                'unique_mail_types': len(self.mail_by_type)
            })
            
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Failed to create sample data", e)
            return False
    def combine_and_process_data(self) -> bool:
            """
            Combine all data sources and create analysis-ready dataset
            """
            self.logger.info("üîÑ Combining and processing data...")
            
            try:
                # Start with call volume data as primary source
                if not self.has_call_volume_data:
                    self.logger.error("‚ùå No call volume data available for combination")
                    return False
                
                # Use call volume as base
                combined = self.call_volume_df.copy()
                
                # Add mail data (inner join to only keep dates with both call and mail data)
                if self.has_mail_data:
                    # Aggregate mail data by date
                    daily_mail = (self.mail_df
                                .groupby('date')['volume']
                                .sum()
                                .reset_index()
                                .rename(columns={'volume': 'mail_volume'}))
                    
                    if self.config['USE_INNER_JOIN']:
                        combined = pd.merge(combined, daily_mail, on='date', how='inner')
                        self.logger.info(f"Inner join result: {len(combined)} records with both call and mail data")
                    else:
                        combined = pd.merge(combined, daily_mail, on='date', how='left')
                        combined['mail_volume'] = combined['mail_volume'].fillna(0)
                        self.logger.info(f"Left join result: {len(combined)} records")
                else:
                    combined['mail_volume'] = 0
                    self.logger.warning("No mail data available, using zero mail volume")
                
                # Add financial data
                if self.has_financial_data:
                    combined = pd.merge(combined, self.financial_df, on='date', how='left')
                    
                    # Forward fill financial data for missing dates
                    financial_cols = [col for col in self.financial_df.columns if col != 'date']
                    for col in financial_cols:
                        if col in combined.columns:
                            combined[col] = combined[col].fillna(method='ffill').fillna(method='bfill')
                            # Apply z-score normalization for financial data
                            if combined[col].std() > 0:
                                combined[f'{col}_normalized'] = zscore(combined[col], nan_policy='omit')
                            else:
                                combined[f'{col}_normalized'] = 0
                    
                    self.logger.info(f"Added {len(financial_cols)} financial indicators")
                
                # Calculate momentum and acceleration features
                for window in self.config['MOMENTUM_WINDOWS']:
                    # Mail momentum (percentage change)
                    combined[f'mail_momentum_{window}d'] = combined['mail_volume'].pct_change(periods=window) * 100
                    combined[f'mail_momentum_{window}d'] = combined[f'mail_momentum_{window}d'].fillna(0)
                    
                    # Mail acceleration (change in momentum)
                    combined[f'mail_acceleration_{window}d'] = combined[f'mail_momentum_{window}d'].diff()
                    combined[f'mail_acceleration_{window}d'] = combined[f'mail_acceleration_{window}d'].fillna(0)
                    
                    # Rolling statistics
                    combined[f'mail_rolling_mean_{window}d'] = combined['mail_volume'].rolling(window=window).mean()
                    combined[f'mail_rolling_std_{window}d'] = combined['mail_volume'].rolling(window=window).std()
                    combined[f'call_rolling_mean_{window}d'] = combined['call_volume'].rolling(window=window).mean()
                    combined[f'call_rolling_std_{window}d'] = combined['call_volume'].rolling(window=window).std()
                
                # Add temporal features
                combined['weekday'] = combined['date'].dt.day_name()
                combined['day_of_week'] = combined['date'].dt.weekday
                combined['month'] = combined['date'].dt.month
                combined['quarter'] = combined['date'].dt.quarter
                combined['is_month_end'] = combined['date'].dt.is_month_end
                combined['is_quarter_end'] = combined['date'].dt.is_quarter_end
                
                # Create normalized versions of key metrics
                for col in ['call_volume', 'mail_volume']:
                    if combined[col].std() > 0:
                        combined[f'{col}_normalized'] = zscore(combined[col], nan_policy='omit')
                    else:
                        combined[f'{col}_normalized'] = 0
                
                # Calculate efficiency metrics
                combined['calls_per_1k_mails'] = np.where(
                    combined['mail_volume'] > 0,
                    (combined['call_volume'] / combined['mail_volume']) * 1000,
                    0
                )
                
                # Remove outliers using robust method
                for col in ['call_volume', 'mail_volume']:
                    if col in combined.columns:
                        q1 = combined[col].quantile(0.25)
                        q3 = combined[col].quantile(0.75)
                        iqr = q3 - q1
                        lower_bound = q1 - 1.5 * iqr
                        upper_bound = q3 + 1.5 * iqr
                        
                        outliers = (combined[col] < lower_bound) | (combined[col] > upper_bound)
                        outlier_count = outliers.sum()
                        
                        if outlier_count > 0:
                            combined = combined[~outliers]
                            self.data_quality_metrics['outliers_removed'] += outlier_count
                            self.logger.info(f"Removed {outlier_count} outliers from {col}")
                
                # Sort by date and reset index
                combined = combined.sort_values('date').reset_index(drop=True)
                
                # Final validation
                if len(combined) < self.config['MIN_SAMPLE_SIZE']:
                    self.logger.error(f"Insufficient combined data: {len(combined)} records")
                    return False
                
                self.combined_df = combined
                self.data_quality_metrics['records_after_filtering'] = len(combined)
                
                self.logger.info("‚úÖ Data combination completed successfully", {
                    'final_records': len(combined),
                    'date_range': f"{combined['date'].min().date()} to {combined['date'].max().date()}",
                    'features_created': len([col for col in combined.columns if any(x in col for x in ['momentum', 'acceleration', 'rolling'])]),
                    'data_quality': self.data_quality_metrics
                })
                
                return True
                
            except Exception as e:
                self.logger.error("‚ùå Data combination failed", e)
                return False
        
    def analyze_enhanced_correlations(self) -> bool:
        """
        Perform comprehensive correlation analysis with enhanced features
        """
        if not SCIPY_AVAILABLE or self.combined_df.empty:
            self.logger.warning("Enhanced correlation analysis skipped - missing dependencies or data")
            return False
        
        self.logger.info("üîÑ Analyzing enhanced correlations...")
        
        try:
            df = self.combined_df
            max_lag = self.config['MAX_LAG_DAYS']
            all_correlations = []
            
            # Test different feature types
            feature_categories = {
                'raw_volume': ['mail_volume'],
                'momentum': [col for col in df.columns if 'momentum' in col and 'mail' in col],
                'acceleration': [col for col in df.columns if 'acceleration' in col and 'mail' in col],
                'rolling_mean': [col for col in df.columns if 'rolling_mean' in col and 'mail' in col],
                'normalized': [col for col in df.columns if 'normalized' in col and 'mail' in col]
            }
            
            for category, features in feature_categories.items():
                for feature in features:
                    if feature not in df.columns:
                        continue
                    
                    # Clean feature data
                    feature_data = df[feature].replace([np.inf, -np.inf], np.nan).dropna()
                    
                    if len(feature_data) < self.config['MIN_SAMPLE_SIZE']:
                        continue
                    
                    # Test different lag periods
                    for lag in range(0, max_lag + 1, 2):  # Test every 2 days
                        try:
                            if lag == 0:
                                x_data = df[feature]
                                y_data = df['call_volume']
                            else:
                                x_data = df[feature]
                                y_data = df['call_volume'].shift(-lag)
                            
                            # Create valid dataset
                            valid_df = pd.DataFrame({
                                'x': x_data,
                                'y': y_data
                            }).replace([np.inf, -np.inf], np.nan).dropna()
                            
                            if len(valid_df) >= self.config['MIN_SAMPLE_SIZE']:
                                # Calculate Pearson correlation
                                corr, p_val = pearsonr(valid_df['x'], valid_df['y'])
                                
                                if np.isfinite(corr):
                                    all_correlations.append({
                                        'feature': feature,
                                        'category': category,
                                        'lag_days': lag,
                                        'correlation': corr,
                                        'p_value': p_val,
                                        'significant': p_val < self.config['SIGNIFICANCE_LEVEL'],
                                        'sample_size': len(valid_df),
                                        'abs_correlation': abs(corr)
                                    })
                                    
                                    # Log significant findings
                                    if abs(corr) > self.config['MIN_CORRELATION_THRESHOLD'] and p_val < self.config['SIGNIFICANCE_LEVEL']:
                                        self.logger.info(f"Significant correlation found: {feature} lag {lag}d = {corr:.3f} (p={p_val:.3f})")
                        
                        except Exception as e:
                            self.logger.debug(f"Correlation calculation failed for {feature} lag {lag}: {e}")
                            continue
            
            # Store results
            if all_correlations:
                self.momentum_correlation_results = pd.DataFrame(all_correlations)
                self.momentum_correlation_results = self.momentum_correlation_results.sort_values(
                    'abs_correlation', ascending=False
                )
                
                # Create data table for display (top correlations)
                self.correlation_data_table = self.momentum_correlation_results.head(20).copy()
                
                self.logger.info("‚úÖ Enhanced correlation analysis completed", {
                    'total_correlations': len(all_correlations),
                    'significant_correlations': len(self.momentum_correlation_results[
                        self.momentum_correlation_results['significant']
                    ]),
                    'best_correlation': self.momentum_correlation_results.iloc[0]['correlation'],
                    'best_feature': self.momentum_correlation_results.iloc[0]['feature']
                })
                
                return True
            else:
                self.logger.warning("No valid correlations found")
                return False
                
        except Exception as e:
            self.logger.error("‚ùå Enhanced correlation analysis failed", e)
            return False

    def analyze_rolling_correlation(self) -> bool:
        """
        Analyze rolling correlation patterns over time
        """
        if not SCIPY_AVAILABLE or self.combined_df.empty:
            self.logger.warning("Rolling correlation analysis skipped")
            return False
        
        self.logger.info("üîÑ Analyzing rolling correlation patterns...")
        
        try:
            df = self.combined_df
            window_size = self.config['ROLLING_WINDOW_SIZE']
            
            if len(df) < window_size + 10:
                self.logger.warning(f"Insufficient data for rolling correlation: need {window_size + 10}, have {len(df)}")
                return False
            
            rolling_results = []
            
            # Test different lag periods
            test_lags = [0, 1, 3, 5, 7, 14]
            
            for lag in test_lags:
                correlations = []
                dates = []
                
                for i in range(window_size, len(df) - max(lag, 1)):
                    window_data = df.iloc[i-window_size:i].copy()
                    
                    if len(window_data) >= 20:  # Minimum window size
                        try:
                            if lag == 0:
                                x_data = window_data['mail_volume']
                                y_data = window_data['call_volume']
                            else:
                                # Use mail data to predict future calls
                                future_data = df.iloc[i-window_size+lag:i+lag].copy()
                                if len(future_data) >= 20:
                                    x_data = window_data['mail_volume']
                                    y_data = future_data['call_volume']
                                else:
                                    correlations.append(0)
                                    dates.append(df.iloc[i]['date'])
                                    continue
                            
                            # Calculate correlation
                            valid_data = pd.DataFrame({
                                'x': x_data,
                                'y': y_data
                            }).dropna()
                            
                            if len(valid_data) >= 15 and valid_data['x'].std() > 0 and valid_data['y'].std() > 0:
                                corr, _ = pearsonr(valid_data['x'], valid_data['y'])
                                correlations.append(corr if np.isfinite(corr) else 0)
                            else:
                                correlations.append(0)
                                
                            dates.append(df.iloc[i]['date'])
                            
                        except Exception as e:
                            correlations.append(0)
                            dates.append(df.iloc[i]['date'])
                            continue
                
                # Store results for this lag
                for date, corr in zip(dates, correlations):
                    rolling_results.append({
                        'date': date,
                        'lag_days': lag,
                        'correlation': corr,
                        'window_size': window_size
                    })
            
            if rolling_results:
                self.rolling_correlation_results = pd.DataFrame(rolling_results)
                
                # Calculate summary statistics
                summary_stats = {}
                for lag in test_lags:
                    lag_data = self.rolling_correlation_results[
                        self.rolling_correlation_results['lag_days'] == lag
                    ]
                    if not lag_data.empty:
                        summary_stats[f'lag_{lag}d'] = {
                            'mean': lag_data['correlation'].mean(),
                            'std': lag_data['correlation'].std(),
                            'max': lag_data['correlation'].max(),
                            'min': lag_data['correlation'].min()
                        }
                
                self.logger.info("‚úÖ Rolling correlation analysis completed", {
                    'total_windows': len(rolling_results),
                    'lags_tested': len(test_lags),
                    'window_size': window_size,
                    'summary_stats': summary_stats
                })
                
                return True
            else:
                self.logger.warning("No rolling correlation results generated")
                return False
                
        except Exception as e:
            self.logger.error("‚ùå Rolling correlation analysis failed", e)
            return False

    def analyze_intent_correlation(self) -> bool:
        """
        Analyze correlation between mail volume and call intents across different lags
        """
        if not self.has_call_intent_data or not SCIPY_AVAILABLE or self.combined_df.empty:
            self.logger.warning("Intent correlation analysis skipped - missing data or dependencies")
            return False
        
        self.logger.info("üîÑ Analyzing intent-level correlations...")
        
        try:
            # Create daily intent counts
            daily_intents = (self.call_intent_df
                            .groupby(['date', 'intent'])
                            .size()
                            .reset_index(name='count'))
            
            # Pivot to get intent columns
            intent_pivot = daily_intents.pivot(index='date', columns='intent', values='count').fillna(0)
            
            if intent_pivot.empty:
                self.logger.warning("No intent data available for correlation analysis")
                return False
            
            # Get mail volume data aligned with intent dates
            mail_data = self.combined_df[['date', 'mail_volume']].copy()
            
            # Merge with intent data
            intent_mail_data = pd.merge(
                intent_pivot.reset_index(),
                mail_data,
                on='date',
                how='inner'
            )
            
            if len(intent_mail_data) < self.config['MIN_SAMPLE_SIZE']:
                self.logger.warning("Insufficient overlapping data for intent correlation")
                return False
            
            # Calculate correlation matrix across different lags
            max_lag = self.config['MAX_LAG_DAYS']
            correlation_matrix = []
            
            for intent in intent_pivot.columns:
                intent_correlations = {'intent': intent}
                
                for lag in range(0, max_lag + 1, 2):  # Test every 2 days
                    try:
                        if lag == 0:
                            intent_data = intent_mail_data[intent]
                            mail_data_aligned = intent_mail_data['mail_volume']
                        else:
                            # Shift intent data to test if mail predicts future calls
                            intent_data = intent_mail_data[intent].shift(-lag)
                            mail_data_aligned = intent_mail_data['mail_volume']
                        
                        # Create valid dataset
                        valid_data = pd.DataFrame({
                            'intent': intent_data,
                            'mail': mail_data_aligned
                        }).dropna()
                        
                        if (len(valid_data) >= 15 and 
                            valid_data['mail'].std() > 0 and 
                            valid_data['intent'].std() > 0):
                            
                            corr, p_val = pearsonr(valid_data['mail'], valid_data['intent'])
                            
                            if np.isfinite(corr):
                                intent_correlations[f'lag_{lag}'] = corr
                                
                                # Log significant correlations
                                if abs(corr) > 0.2 and p_val < 0.05:
                                    self.logger.info(f"Strong intent correlation: {intent} lag {lag}d = {corr:.3f}")
                            else:
                                intent_correlations[f'lag_{lag}'] = 0
                        else:
                            intent_correlations[f'lag_{lag}'] = 0
                    
                    except Exception as e:
                        intent_correlations[f'lag_{lag}'] = 0
                        self.logger.debug(f"Intent correlation calculation failed: {e}")
                
                correlation_matrix.append(intent_correlations)
            
            # Store results
            if correlation_matrix:
                self.intent_correlation_results = pd.DataFrame(correlation_matrix)
                
                # Calculate summary statistics
                lag_cols = [col for col in self.intent_correlation_results.columns if col.startswith('lag_')]
                summary_stats = {}
                
                for col in lag_cols:
                    summary_stats[col] = {
                        'mean': self.intent_correlation_results[col].mean(),
                        'max': self.intent_correlation_results[col].max(),
                        'min': self.intent_correlation_results[col].min(),
                        'std': self.intent_correlation_results[col].std()
                    }
                
                self.logger.info("‚úÖ Intent correlation analysis completed", {
                    'intents_analyzed': len(correlation_matrix),
                    'lags_tested': len(lag_cols),
                    'summary_stats': summary_stats
                })
                
                return True
            else:
                self.logger.warning("No intent correlations calculated")
                return False
                
        except Exception as e:
            self.logger.error("‚ùå Intent correlation analysis failed", e)
            return False

    def calculate_efficiency_metrics(self) -> bool:
        """
        Calculate comprehensive efficiency and performance metrics
        """
        try:
            if self.combined_df.empty:
                self.logger.warning("No data available for efficiency calculations")
                return False
            
            df = self.combined_df
            
            # Basic volume metrics
            total_calls = df['call_volume'].sum()
            total_mail = df['mail_volume'].sum()
            avg_daily_calls = df['call_volume'].mean()
            avg_daily_mail = df['mail_volume'].mean()
            
            # Response rate
            response_rate = (total_calls / total_mail * 100) if total_mail > 0 else 0
            
            # Efficiency trends
            if len(df) >= 30:
                recent_15 = df.tail(15)['call_volume'].mean()
                previous_15 = df.iloc[-30:-15]['call_volume'].mean()
                call_trend = ((recent_15 - previous_15) / previous_15 * 100) if previous_15 > 0 else 0
                
                recent_15_mail = df.tail(15)['mail_volume'].mean()
                previous_15_mail = df.iloc[-30:-15]['mail_volume'].mean()
                mail_trend = ((recent_15_mail - previous_15_mail) / previous_15_mail * 100) if previous_15_mail > 0 else 0
            else:
                call_trend = 0
                mail_trend = 0
            
            # Volatility measures
            call_volatility = df['call_volume'].std() / df['call_volume'].mean() * 100 if df['call_volume'].mean() > 0 else 0
            mail_volatility = df['mail_volume'].std() / df['mail_volume'].mean() * 100 if df['mail_volume'].mean() > 0 else 0
            
            # Correlation-based metrics
            best_correlation = 0
            best_lag = 0
            
            if not self.momentum_correlation_results.empty:
                best_row = self.momentum_correlation_results.iloc[0]
                best_correlation = best_row['correlation']
                best_lag = best_row['lag_days']
            
            # Efficiency ratios
            avg_calls_per_1k_mails = df['calls_per_1k_mails'].mean() if 'calls_per_1k_mails' in df.columns else 0
            
            self.efficiency_metrics = {
                'total_calls': int(total_calls),
                'total_mail': int(total_mail),
                'avg_daily_calls': round(avg_daily_calls, 1),
                'avg_daily_mail': round(avg_daily_mail, 0),
                'response_rate_pct': round(response_rate, 2),
                'call_trend_15d': round(call_trend, 1),
                'mail_trend_15d': round(mail_trend, 1),
                'call_volatility': round(call_volatility, 1),
                'mail_volatility': round(mail_volatility, 1),
                'best_correlation': round(best_correlation, 3),
                'best_lag_days': int(best_lag),
                'avg_calls_per_1k_mails': round(avg_calls_per_1k_mails, 1),
                'data_quality_score': self._calculate_data_quality_score()
            }
            
            self.logger.info("‚úÖ Efficiency metrics calculated", {
                'metrics_calculated': len(self.efficiency_metrics),
                'response_rate': f"{response_rate:.2f}%",
                'best_correlation': best_correlation,
                'data_quality_score': self.efficiency_metrics['data_quality_score']
            })
            
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Efficiency calculation failed", e)
            return False

    def _calculate_data_quality_score(self) -> float:
        """
        Calculate an overall data quality score (0-100)
        """
        try:
            score = 100
            
            # Penalize for missing data
            if not self.has_call_volume_data:
                score -= 30
            if not self.has_mail_data:
                score -= 30
            if not self.has_call_intent_data:
                score -= 10
            if not self.has_financial_data:
                score -= 5
            
            # Penalize for data quality issues
            total_processed = self.data_quality_metrics.get('total_records_processed', 1)
            outliers_removed = self.data_quality_metrics.get('outliers_removed', 0)
            
            if total_processed > 0:
                outlier_rate = outliers_removed / total_processed
                score -= min(outlier_rate * 100, 15)  # Max 15 points penalty
            
            # Bonus for good sample size
            if len(self.combined_df) >= 100:
                score += 5
            
            return max(0, min(100, score))
            
        except Exception:
            return 50  # Default score if calculation fails
    def master_data_pipeline(self) -> bool:
            """
            Master data processing pipeline - orchestrates all data operations
            """
            self.logger.info("üöÄ Starting master data processing pipeline...")
            
            try:
                # Phase 1: Data Loading
                self.logger.info("üì• Phase 1: Data Loading")
                
                # Load call volume data (primary)
                call_volume_success = self.load_call_volume_data()
                
                # Load call intent data (secondary)
                call_intent_success = self.load_call_intent_data()
                
                # Load mail data
                mail_success = self.load_mail_data()
                
                # Load financial data
                financial_success = self.load_financial_data()
                
                # Check if we have minimum required data
                if not call_volume_success and not call_intent_success:
                    self.logger.warning("‚ö†Ô∏è No call data available, creating sample data...")
                    if not self.create_sample_data():
                        self.logger.error("‚ùå Failed to create sample data")
                        return False
                
                # Phase 2: Data Combination and Processing
                self.logger.info("‚öôÔ∏è Phase 2: Data Combination and Processing")
                
                if not self.combine_and_process_data():
                    self.logger.error("‚ùå Data combination failed")
                    return False
                
                # Phase 3: Advanced Analytics
                self.logger.info("üî¨ Phase 3: Advanced Analytics")
                
                # Enhanced correlation analysis
                correlation_success = self.analyze_enhanced_correlations()
                
                # Rolling correlation analysis
                rolling_success = self.analyze_rolling_correlation()
                
                # Intent correlation analysis (if data available)
                intent_success = self.analyze_intent_correlation()
                
                # Efficiency metrics
                efficiency_success = self.calculate_efficiency_metrics()
                
                # Phase 4: Advanced Features
                self.logger.info("üéØ Phase 4: Advanced Features")
                
                # Anomaly detection
                anomaly_success = self.detect_anomalies()
                
                # Time series decomposition
                decomposition_success = self.perform_time_series_decomposition()
                
                # Final status report
                self.logger.info("=" * 70)
                self.logger.info("üéâ MASTER DATA PIPELINE COMPLETED")
                self.logger.info("=" * 70)
                
                pipeline_results = {
                    'data_loading': {
                        'call_volume': call_volume_success,
                        'call_intent': call_intent_success,
                        'mail': mail_success,
                        'financial': financial_success
                    },
                    'data_processing': {
                        'combination': True,
                        'records_processed': len(self.combined_df)
                    },
                    'analytics': {
                        'correlation': correlation_success,
                        'rolling_correlation': rolling_success,
                        'intent_correlation': intent_success,
                        'efficiency_metrics': efficiency_success
                    },
                    'advanced_features': {
                        'anomaly_detection': anomaly_success,
                        'time_series_decomposition': decomposition_success
                    },
                    'data_quality': self.data_quality_metrics
                }
                
                self.logger.info("Pipeline Results:", pipeline_results)
                
                return True
                
            except Exception as e:
                self.logger.error("‚ùå Master data pipeline failed", e)
                return False
                
    def detect_anomalies(self) -> bool:
            """
            Detect anomalies in mail-call relationship patterns
            """
            if not SKLEARN_AVAILABLE or self.combined_df.empty:
                self.logger.warning("Anomaly detection skipped - missing dependencies or data")
                return False
            
            self.logger.info("üîÑ Detecting anomalies in communication patterns...")
            
            try:
                df = self.combined_df.copy()
                
                # Prepare features for anomaly detection
                features = ['call_volume', 'mail_volume', 'calls_per_1k_mails']
                
                # Add momentum features if available
                momentum_features = [col for col in df.columns if 'momentum' in col and 'mail' in col]
                features.extend(momentum_features[:2])  # Add top 2 momentum features
                
                # Filter to available features
                available_features = [f for f in features if f in df.columns]
                
                if len(available_features) < 2:
                    self.logger.warning("Insufficient features for anomaly detection")
                    return False
                
                # Prepare feature matrix
                feature_matrix = df[available_features].copy()
                
                # Handle missing values
                feature_matrix = feature_matrix.fillna(feature_matrix.mean())
                
                # Normalize features
                scaler = StandardScaler()
                feature_matrix_scaled = scaler.fit_transform(feature_matrix)
                
                # Detect anomalies using Isolation Forest
                iso_forest = IsolationForest(
                    contamination=0.1,  # Expect 10% anomalies
                    random_state=42,
                    n_estimators=100
                )
                
                anomaly_labels = iso_forest.fit_predict(feature_matrix_scaled)
                anomaly_scores = iso_forest.decision_function(feature_matrix_scaled)
                
                # Add results to dataframe
                df['anomaly_label'] = anomaly_labels
                df['anomaly_score'] = anomaly_scores
                df['is_anomaly'] = anomaly_labels == -1
                
                # Extract anomaly records
                anomalies = df[df['is_anomaly']].copy()
                
                if not anomalies.empty:
                    # Sort by anomaly score (most anomalous first)
                    anomalies = anomalies.sort_values('anomaly_score').copy()
                    
                    # Add anomaly reasons
                    anomaly_reasons = []
                    for _, row in anomalies.iterrows():
                        reasons = []
                        
                        # Check for volume anomalies
                        if row['call_volume'] > df['call_volume'].quantile(0.95):
                            reasons.append("High call volume")
                        elif row['call_volume'] < df['call_volume'].quantile(0.05):
                            reasons.append("Low call volume")
                        
                        if row['mail_volume'] > df['mail_volume'].quantile(0.95):
                            reasons.append("High mail volume")
                        elif row['mail_volume'] < df['mail_volume'].quantile(0.05):
                            reasons.append("Low mail volume")
                        
                        # Check for ratio anomalies
                        if 'calls_per_1k_mails' in row and row['calls_per_1k_mails'] > df['calls_per_1k_mails'].quantile(0.95):
                            reasons.append("High call-to-mail ratio")
                        
                        anomaly_reasons.append("; ".join(reasons) if reasons else "Pattern anomaly")
                    
                    anomalies['anomaly_reason'] = anomaly_reasons
                    
                    self.anomaly_data = anomalies[['date', 'call_volume', 'mail_volume', 'anomaly_score', 'anomaly_reason']].copy()
                    
                    self.logger.info("‚úÖ Anomaly detection completed", {
                        'total_anomalies': len(anomalies),
                        'anomaly_rate': f"{len(anomalies)/len(df)*100:.1f}%",
                        'most_anomalous_date': anomalies.iloc[0]['date'].strftime('%Y-%m-%d'),
                        'features_used': available_features
                    })
                    
                    return True
                else:
                    self.logger.info("No anomalies detected in the data")
                    return False
                    
            except Exception as e:
                self.logger.error("‚ùå Anomaly detection failed", e)
                return False
        
    def perform_time_series_decomposition(self) -> bool:
        """
        Perform time series decomposition for trend, seasonal, and residual components
        """
        if not STATSMODELS_AVAILABLE or self.combined_df.empty:
            self.logger.warning("Time series decomposition skipped - missing dependencies or data")
            return False
        
        self.logger.info("üîÑ Performing time series decomposition...")
        
        try:
            df = self.combined_df.copy()
            
            if len(df) < 50:  # Need enough data for decomposition
                self.logger.warning("Insufficient data for time series decomposition")
                return False
            
            # Set date as index
            df = df.set_index('date').sort_index()
            
            decomposition_results = {}
            
            # Decompose call volume
            try:
                call_decomp = seasonal_decompose(
                    df['call_volume'], 
                    model='additive',
                    period=7,  # Weekly seasonality
                    extrapolate_trend='freq'
                )
                
                decomposition_results['call_volume'] = {
                    'trend': call_decomp.trend,
                    'seasonal': call_decomp.seasonal,
                    'residual': call_decomp.resid,
                    'observed': call_decomp.observed
                }
                
                self.logger.info("‚úÖ Call volume decomposition completed")
                
            except Exception as e:
                self.logger.warning(f"Call volume decomposition failed: {e}")
            
            # Decompose mail volume
            try:
                mail_decomp = seasonal_decompose(
                    df['mail_volume'], 
                    model='additive',
                    period=7,  # Weekly seasonality
                    extrapolate_trend='freq'
                )
                
                decomposition_results['mail_volume'] = {
                    'trend': mail_decomp.trend,
                    'seasonal': mail_decomp.seasonal,
                    'residual': mail_decomp.resid,
                    'observed': mail_decomp.observed
                }
                
                self.logger.info("‚úÖ Mail volume decomposition completed")
                
            except Exception as e:
                self.logger.warning(f"Mail volume decomposition failed: {e}")
            
            if decomposition_results:
                self.decomposition_results = decomposition_results
                
                # Calculate decomposition statistics
                stats = {}
                for series_name, components in decomposition_results.items():
                    stats[series_name] = {
                        'trend_strength': components['trend'].std() / components['observed'].std(),
                        'seasonal_strength': components['seasonal'].std() / components['observed'].std(),
                        'residual_strength': components['residual'].std() / components['observed'].std()
                    }
                
                self.logger.info("‚úÖ Time series decomposition completed", {
                    'series_decomposed': len(decomposition_results),
                    'decomposition_stats': stats
                })
                
                return True
            else:
                self.logger.warning("No successful decompositions completed")
                return False
                
        except Exception as e:
            self.logger.error("‚ùå Time series decomposition failed", e)
            return False

    def master_data_pipeline(self) -> bool:
        """
        Master data processing pipeline - orchestrates all data operations
        """
        self.logger.info("üöÄ Starting master data processing pipeline...")
        
        try:
            # Phase 1: Data Loading
            self.logger.info("üì• Phase 1: Data Loading")
            
            # Load call volume data (primary)
            call_volume_success = self.load_call_volume_data()
            
            # Load call intent data (secondary)
            call_intent_success = self.load_call_intent_data()
            
            # Load mail data
            mail_success = self.load_mail_data()
            
            # Load financial data
            financial_success = self.load_financial_data()
            
            # Check if we have minimum required data
            if not call_volume_success and not call_intent_success:
                self.logger.warning("‚ö†Ô∏è No call data available, creating sample data...")
                if not self.create_sample_data():
                    self.logger.error("‚ùå Failed to create sample data")
                    return False
            
            # Phase 2: Data Combination and Processing
            self.logger.info("‚öôÔ∏è Phase 2: Data Combination and Processing")
            
            if not self.combine_and_process_data():
                self.logger.error("‚ùå Data combination failed")
                return False
            
            # Phase 3: Advanced Analytics
            self.logger.info("üî¨ Phase 3: Advanced Analytics")
            
            # Enhanced correlation analysis
            correlation_success = self.analyze_enhanced_correlations()
            
            # Rolling correlation analysis
            rolling_success = self.analyze_rolling_correlation()
            
            # Intent correlation analysis (if data available)
            intent_success = self.analyze_intent_correlation()
            
            # Efficiency metrics
            efficiency_success = self.calculate_efficiency_metrics()
            
            # Phase 4: Advanced Features
            self.logger.info("üéØ Phase 4: Advanced Features")
            
            # Anomaly detection
            anomaly_success = self.detect_anomalies()
            
            # Time series decomposition
            decomposition_success = self.perform_time_series_decomposition()
            
            # Final status report
            self.logger.info("=" * 70)
            self.logger.info("üéâ MASTER DATA PIPELINE COMPLETED")
            self.logger.info("=" * 70)
            
            pipeline_results = {
                'data_loading': {
                    'call_volume': call_volume_success,
                    'call_intent': call_intent_success,
                    'mail': mail_success,
                    'financial': financial_success
                },
                'data_processing': {
                    'combination': True,
                    'records_processed': len(self.combined_df)
                },
                'analytics': {
                    'correlation': correlation_success,
                    'rolling_correlation': rolling_success,
                    'intent_correlation': intent_success,
                    'efficiency_metrics': efficiency_success
                },
                'advanced_features': {
                    'anomaly_detection': anomaly_success,
                    'time_series_decomposition': decomposition_success
                },
                'data_quality': self.data_quality_metrics
            }
            
            self.logger.info("Pipeline Results:", pipeline_results)
            
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Master data pipeline failed", e)
            return False

# =============================================================================
# PREMIUM DASHBOARD VISUALIZATIONS v3.0
# =============================================================================

class PremiumDashboardVisualizations:

    def __init__(self, data_processor: EnterpriseDataProcessor):
        self.dp = data_processor
        self.logger = data_processor.logger
        self.theme = THEME
        self.config = ENTERPRISE_CONFIG
        
        # Chart configuration
        self.chart_config = {
            'displayModeBar': True,
            'displaylogo': False,
            'modeBarButtonsToRemove': ['pan2d', 'lasso2d', 'select2d'],
            'toImageButtonOptions': {
                'format': 'png',
                'filename': 'customer_communications_chart',
                'height': 800,
                'width': 1200,
                'scale': 2
            }
        }
        
        # Common layout settings
        self.base_layout = {
            'template': 'plotly_white',
            'font': {
                'family': self.theme.font_family,
                'size': 14,
                'color': self.theme.neutral_700
            },
            'title': {
                'font': {
                    'family': self.theme.font_display,
                    'size': 24,
                    'color': self.theme.neutral_900
                },
                'x': 0.5,
                'xanchor': 'center'
            },
            'legend': {
                'orientation': 'h',
                'yanchor': 'bottom',
                'y': 1.02,
                'xanchor': 'center',
                'x': 0.5,
                'font': {'size': 13}
            },
            'margin': self.config['CHART_MARGIN'],
            'hovermode': 'closest',
            'hoverdistance': self.config['HOVER_DISTANCE']
        }

    def create_error_figure(self, message: str, height: int = 600) -> go.Figure:
        """
        Create elegant error figure with premium styling
        """
        fig = go.Figure()
        
        fig.add_annotation(
            text=f"‚ö†Ô∏è {message}",
            xref="paper", yref="paper",
            x=0.5, y=0.5,
            showarrow=False,
            font={
                'size': 20,
                'color': self.theme.error,
                'family': self.theme.font_family
            },
            bgcolor=self.theme.neutral_50,
            bordercolor=self.theme.error,
            borderwidth=2,
            borderpad=20
        )
        
        fig.update_layout(
            **self.base_layout,
            height=height,
            showlegend=False,
            xaxis={'showgrid': False, 'showticklabels': False},
            yaxis={'showgrid': False, 'showticklabels': False}
        )
        
        return fig

    def create_premium_overview_chart(self, df: pd.DataFrame) -> go.Figure:
        """
        Create ultra-premium executive overview chart with enhanced styling
        """
        if df.empty:
            return self.create_error_figure("No data available for overview analysis")
        
        try:
            # Create sophisticated subplot layout
            fig = make_subplots(
                rows=2, cols=1,
                subplot_titles=('üìä Executive Overview: Communications Intelligence & Market Context',
                                'üìà Daily Volume Analysis with Trend Indicators'),
                vertical_spacing=0.15,
                specs=[[{"secondary_y": True}], [{"secondary_y": True}]],
                row_heights=[0.6, 0.4]
            )
            
            # TOP CHART: Normalized trends with financial context
            
            # Main communication trends
            fig.add_trace(
                go.Scatter(
                    x=df['date'],
                    y=df['call_volume_normalized'] if 'call_volume_normalized' in df.columns else df['call_volume'],
                    name='Call Volume',
                    line={
                        'color': self.theme.primary_rose,
                        'width': 4,
                        'shape': 'spline'
                    },
                    mode='lines',
                    hovertemplate='<b>Call Volume</b><br>Date: %{x}<br>Normalized: %{y:.2f}<extra></extra>'
                ),
                row=1, col=1, secondary_y=False
            )
            
            fig.add_trace(
                go.Scatter(
                    x=df['date'],
                    y=df['mail_volume_normalized'] if 'mail_volume_normalized' in df.columns else df['mail_volume'],
                    name='Mail Volume',
                    line={
                        'color': self.theme.primary_blue,
                        'width': 4,
                        'shape': 'spline'
                    },
                    mode='lines',
                    hovertemplate='<b>Mail Volume</b><br>Date: %{x}<br>Normalized: %{y:.2f}<extra></extra>'
                ),
                row=1, col=1, secondary_y=False
            )
            
            # Financial context (properly normalized)
            financial_colors = [self.theme.primary_rose, self.theme.primary_teal, self.theme.accent_gold]
            financial_indicators = ['S&P 500', '10-Year Treasury', 'VIX']
            
            for i, indicator in enumerate(financial_indicators):
                norm_col = f'{indicator}_normalized'
                if norm_col in df.columns and df[norm_col].notna().any():
                    fig.add_trace(
                        go.Scatter(
                            x=df['date'],
                            y=df[norm_col],
                            name=f'{indicator}',
                            line={
                                'color': financial_colors[i % len(financial_colors)],
                                'width': 3,
                                'dash': 'dot'
                            },
                            opacity=0.8,
                            hovertemplate=f'<b>{indicator}</b><br>Date: %{{x}}<br>Normalized: %{{y:.2f}}<extra></extra>'
                        ),
                        row=1, col=1, secondary_y=True
                    )
            
            # BOTTOM CHART: Raw volumes with trend analysis
            
            # Mail volume bars
            fig.add_trace(
                go.Bar(
                    x=df['date'],
                    y=df['mail_volume'],
                    name='Daily Mail Volume',
                    marker={
                        'color': self.theme.primary_blue,
                        'opacity': 0.7,
                        'line': {'width': 0}
                    },
                    hovertemplate='<b>Mail Volume</b><br>Date: %{x}<br>Volume: %{y:,.0f}<extra></extra>'
                ),
                row=2, col=1, secondary_y=False
            )
            
            # Call volume line
            fig.add_trace(
                go.Scatter(
                    x=df['date'],
                    y=df['call_volume'],
                    name='Daily Call Volume',
                    line={
                        'color': self.theme.primary_rose,
                        'width': 5
                    },
                    mode='lines+markers',
                    marker={'size': 8, 'color': self.theme.primary_red},
                    hovertemplate='<b>Call Volume</b><br>Date: %{x}<br>Volume: %{y:,.0f}<extra></extra>'
                ),
                row=2, col=1, secondary_y=True
            )
            
            # Add trend lines if enough data
            if len(df) >= 20:
                # Simple linear trend for calls
                x_numeric = np.arange(len(df))
                call_trend = np.polyfit(x_numeric, df['call_volume'], 1)
                call_trend_line = np.poly1d(call_trend)(x_numeric)
                
                fig.add_trace(
                    go.Scatter(
                        x=df['date'],
                        y=call_trend_line,
                        name='Call Trend',
                        line={
                            'color': self.theme.accent_rose,
                            'width': 3,
                            'dash': 'dash'
                        },
                        showlegend=False
                    ),
                    row=2, col=1, secondary_y=True
                )
            
            # Premium styling
            fig.update_layout(
                title={
                    'text': '<b>üéØ Customer Communications Intelligence Dashboard</b><br><sub>Advanced Analytics with Market Context & Trend Analysis</sub>',
                    'font': {
                        'size': 28,
                        'color': self.theme.neutral_900,
                        'family': self.theme.font_display
                    },
                    'x': 0.5
                },
                height=self.config['CHART_HEIGHT_LARGE'],
                showlegend=True,
                legend={
                    'orientation': 'h',
                    'yanchor': 'bottom',
                    'y': 1.02,
                    'xanchor': 'center',
                    'x': 0.5,
                    'font': {'size': 14},
                    'bgcolor': f"rgba(255,255,255,0.9)",
                    'bordercolor': self.theme.neutral_300,
                    'borderwidth': 1
                },
                **self.base_layout
            )
            
            # Axis labels and styling
            fig.update_yaxes(
                title_text="<b>Normalized Score</b>",
                title_font={'size': 16, 'color': self.theme.neutral_700},
                row=1, col=1, secondary_y=False
            )
            
            fig.update_yaxes(
                title_text="<b>Market Indicators (%)</b>",
                title_font={'size': 16, 'color': self.theme.primary_purple},
                row=1, col=1, secondary_y=True
            )
            
            fig.update_yaxes(
                title_text="<b>Mail Volume</b>",
                title_font={'size': 16, 'color': self.theme.primary_blue},
                row=2, col=1, secondary_y=False
            )
            
            fig.update_yaxes(
                title_text="<b>Call Volume</b>",
                title_font={'size': 16, 'color': self.theme.primary_red},
                row=2, col=1, secondary_y=True
            )
            
            fig.update_xaxes(
                title_text="<b>Date</b>",
                title_font={'size': 16},
                row=2, col=1
            )
            
            return fig
            
        except Exception as e:
            self.logger.error("‚ùå Premium overview chart creation failed", e)
            return self.create_error_figure("Failed to create overview chart")

    def create_lag_analysis_heatmap(self, df: pd.DataFrame) -> go.Figure:
        """
        NEW PLOT 1: Lag Analysis Heatmap - Mail types vs lag days correlation
        """
        if df.empty or not self.dp.has_mail_data:
            return self.create_error_figure("Mail type data not available for lag analysis")
        
        try:
            # Calculate correlations between mail types and call volume across lags
            correlation_matrix = []
            lag_days = list(range(0, 15))  # 0 to 14 days
            
            # Get mail types
            mail_types = self.dp.mail_by_type['type'].tolist()[:10]  # Top 10 types
            
            # Calculate daily volumes by mail type
            if 'type' in self.dp.mail_df.columns:
                mail_type_daily = self.dp.mail_df.pivot_table(
                    index='date', 
                    columns='type', 
                    values='volume', 
                    aggfunc='sum'
                ).fillna(0)
            else:
                return self.create_error_figure("Mail type information not available")
            
            # Merge with call data
            combined_data = pd.merge(
                mail_type_daily.reset_index(),
                df[['date', 'call_volume']],
                on='date',
                how='inner'
            )
            
            if len(combined_data) < 20:
                return self.create_error_figure("Insufficient data for lag analysis")
            
            # Calculate correlations
            for mail_type in mail_types:
                if mail_type in combined_data.columns:
                    type_correlations = []
                    
                    for lag in lag_days:
                        try:
                            if lag == 0:
                                x_data = combined_data[mail_type]
                                y_data = combined_data['call_volume']
                            else:
                                x_data = combined_data[mail_type]
                                y_data = combined_data['call_volume'].shift(-lag)
                            
                            # Calculate correlation
                            valid_data = pd.DataFrame({'x': x_data, 'y': y_data}).dropna()
                            
                            if len(valid_data) >= 10 and valid_data['x'].std() > 0 and valid_data['y'].std() > 0:
                                corr, _ = pearsonr(valid_data['x'], valid_data['y'])
                                type_correlations.append(corr if np.isfinite(corr) else 0)
                            else:
                                type_correlations.append(0)
                        except:
                            type_correlations.append(0)
                    
                    correlation_matrix.append(type_correlations)
            
            if not correlation_matrix:
                return self.create_error_figure("No correlations could be calculated")
            
            # Create heatmap
            fig = go.Figure(data=go.Heatmap(
                z=correlation_matrix,
                x=[f'{lag}d' for lag in lag_days],
                y=mail_types,
                colorscale='RdBu',
                zmid=0,
                colorbar={
                    'title': '<b>Correlation<br>Coefficient</b>',
                    'titlefont': {'size': 16},
                    'tickfont': {'size': 14},
                    'thickness': 20
                },
                hoverongaps=False,
                hovertemplate='<b>%{y}</b><br>Lag: %{x}<br>Correlation: <b>%{z:.3f}</b><extra></extra>'
            ))
            
            fig.update_layout(
                title={
                    'text': '<b>üìä Mail Type Lag Analysis Heatmap</b><br><sub>Correlation between mail types and call volume across time lags</sub>',
                    'font': {'size': 24, 'color': self.theme.neutral_900},
                    'x': 0.5
                },
                xaxis_title="<b>Lag Period</b>",
                yaxis_title="<b>Mail Type</b>",
                height=self.config['CHART_HEIGHT'],
                **self.base_layout
            )
            
            return fig
            
        except Exception as e:
            self.logger.error("‚ùå Lag analysis heatmap creation failed", e)
            return self.create_error_figure("Failed to create lag analysis heatmap")

    def create_scatter_with_trend(self, df: pd.DataFrame) -> go.Figure:
        """
        NEW PLOT 2: Scatter plot with trend lines - Mail vs Call volume
        """
        if df.empty:
            return self.create_error_figure("No data available for scatter analysis")
        
        try:
            # Use next day calls for predictive analysis
            if len(df) < 2:
                return self.create_error_figure("Insufficient data for trend analysis")
            
            # Prepare data
            scatter_data = df.copy()
            scatter_data['next_day_calls'] = scatter_data['call_volume'].shift(-1)
            scatter_data = scatter_data.dropna()
            
            if len(scatter_data) < 10:
                return self.create_error_figure("Insufficient data for scatter analysis")
            
            # Calculate trend line
            x_data = scatter_data['mail_volume'].values
            y_data = scatter_data['next_day_calls'].values
            
            # Linear regression
            if SKLEARN_AVAILABLE:
                lr = LinearRegression()
                lr.fit(x_data.reshape(-1, 1), y_data)
                trend_line = lr.predict(x_data.reshape(-1, 1))
                r2 = r2_score(y_data, trend_line)
                slope = lr.coef_[0]
                intercept = lr.intercept_
            else:
                # Fallback to numpy
                slope, intercept = np.polyfit(x_data, y_data, 1)
                trend_line = slope * x_data + intercept
                r2 = np.corrcoef(x_data, y_data)[0, 1] ** 2
            
            # Create scatter plot
            fig = go.Figure()
            
            # Add scatter points colored by day of week
            colors = px.colors.qualitative.Set3
            
            for i, day in enumerate(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']):
                day_data = scatter_data[scatter_data['weekday'] == day]
                if not day_data.empty:
                    fig.add_trace(go.Scatter(
                        x=day_data['mail_volume'],
                        y=day_data['next_day_calls'],
                        mode='markers',
                        name=day,
                        marker={
                            'size': 12,
                            'color': colors[i % len(colors)],
                            'opacity': 0.8,
                            'line': {'width': 1, 'color': 'white'}
                        },
                        hovertemplate=f'<b>{day}</b><br>Mail Volume: %{{x:,.0f}}<br>Next Day Calls: %{{y:,.0f}}<extra></extra>'
                    ))
            
            # Add trend line
            fig.add_trace(go.Scatter(
                x=x_data,
                y=trend_line,
                mode='lines',
                name=f'Trend Line (R¬≤ = {r2:.3f})',
                line={
                    'color': self.theme.primary_rose,
                    'width': 4,
                    'dash': 'dash'
                },
                hovertemplate='<b>Trend Line</b><br>Mail Volume: %{x:,.0f}<br>Predicted Calls: %{y:,.0f}<extra></extra>'
            ))
            
            # Add confidence intervals if sklearn available
            if SKLEARN_AVAILABLE and len(scatter_data) >= 20:
                # Calculate prediction intervals (simplified)
                residuals = y_data - trend_line
                mse = np.mean(residuals**2)
                std_error = np.sqrt(mse)
                
                fig.add_trace(go.Scatter(
                    x=x_data,
                    y=trend_line + 1.96 * std_error,
                    mode='lines',
                    name='95% Confidence',
                    line={'color': self.theme.neutral_400, 'width': 2, 'dash': 'dot'},
                    showlegend=False
                ))
                
                fig.add_trace(go.Scatter(
                    x=x_data,
                    y=trend_line - 1.96 * std_error,
                    mode='lines',
                    name='95% Confidence',
                    line={'color': self.theme.neutral_400, 'width': 2, 'dash': 'dot'},
                    fill='tonexty',
                    fillcolor=f'rgba({int(self.theme.primary_blue[1:3], 16)}, {int(self.theme.primary_blue[3:5], 16)}, {int(self.theme.primary_blue[5:7], 16)}, 0.1)',
                    showlegend=False
                ))
            
            # Add regression equation annotation
            equation_text = f"y = {slope:.3f}x + {intercept:.1f}<br>R¬≤ = {r2:.3f}"
            
            fig.add_annotation(
                x=0.05,
                y=0.95,
                xref="paper",
                yref="paper",
                text=f"<b>Regression Equation:</b><br>{equation_text}",
                showarrow=False,
                font={'size': 14, 'color': self.theme.neutral_700},
                bgcolor=self.theme.neutral_50,
                bordercolor=self.theme.neutral_300,
                borderwidth=1,
                borderpad=10
            )
            
            fig.update_layout(
                title={
                    'text': '<b>üìà Predictive Scatter Analysis</b><br><sub>Mail Volume vs Next Day Call Volume with Trend Analysis</sub>',
                    'font': {'size': 24, 'color': self.theme.neutral_900},
                    'x': 0.5
                },
                xaxis_title="<b>Mail Volume</b>",
                yaxis_title="<b>Next Day Call Volume</b>",
                height=self.config['CHART_HEIGHT'],
                **self.base_layout
            )
            
            return fig
            
        except Exception as e:
            self.logger.error("‚ùå Scatter with trend creation failed", e)
            return self.create_error_figure("Failed to create scatter trend analysis")
    
    def create_time_series_decomposition(self, df: pd.DataFrame) -> go.Figure:
        """
        NEW PLOT 3: Time Series Decomposition - Trend, Seasonal, Residual components
        """
        if df.empty or not self.dp.decomposition_results:
            return self.create_error_figure("Time series decomposition data not available")
        
        try:
            decomp_results = self.dp.decomposition_results
            
            # Create subplot with 4 rows for call volume decomposition
            fig = make_subplots(
                rows=4, cols=1,
                subplot_titles=(
                    'üìä Original Call Volume',
                    'üìà Trend Component',
                    'üîÑ Seasonal Component',
                    'üìâ Residual Component'
                ),
                vertical_spacing=0.08,
                shared_xaxes=True
            )
            
            if 'call_volume' in decomp_results:
                components = decomp_results['call_volume']
                dates = components['observed'].index
                
                # Original series
                fig.add_trace(
                    go.Scatter(
                        x=dates,
                        y=components['observed'],
                        mode='lines',
                        name='Original',
                        line={'color': self.theme.primary_blue, 'width': 3},
                        hovertemplate='<b>Original</b><br>Date: %{x}<br>Value: %{y:,.0f}<extra></extra>'
                    ),
                    row=1, col=1
                )
                
                # Trend component
                fig.add_trace(
                    go.Scatter(
                        x=dates,
                        y=components['trend'],
                        mode='lines',
                        name='Trend',
                        line={'color': self.theme.primary_rose, 'width': 3},
                        hovertemplate='<b>Trend</b><br>Date: %{x}<br>Value: %{y:,.1f}<extra></extra>'
                    ),
                    row=2, col=1
                )
                
                # Seasonal component
                fig.add_trace(
                    go.Scatter(
                        x=dates,
                        y=components['seasonal'],
                        mode='lines',
                        name='Seasonal',
                        line={'color': self.theme.primary_teal, 'width': 3},
                        hovertemplate='<b>Seasonal</b><br>Date: %{x}<br>Value: %{y:,.1f}<extra></extra>'
                    ),
                    row=3, col=1
                )
                
                # Residual component
                fig.add_trace(
                    go.Scatter(
                        x=dates,
                        y=components['residual'],
                        mode='lines',
                        name='Residual',
                        line={'color': self.theme.neutral_600, 'width': 2},
                        hovertemplate='<b>Residual</b><br>Date: %{x}<br>Value: %{y:,.1f}<extra></extra>'
                    ),
                    row=4, col=1
                )
                
                # Add zero line for residuals
                fig.add_hline(
                    y=0,
                    line_dash="dash",
                    line_color=self.theme.neutral_400,
                    row=4, col=1
                )
            
            fig.update_layout(
                title={
                    'text': '<b>üìä Time Series Decomposition Analysis</b><br><sub>Trend, Seasonal, and Residual Components of Call Volume</sub>',
                    'font': {'size': 24, 'color': self.theme.neutral_900},
                    'x': 0.5
                },
                height=self.config['CHART_HEIGHT_LARGE'],
                showlegend=False,
                **self.base_layout
            )
            
            # Update x-axis for bottom subplot only
            fig.update_xaxes(title_text="<b>Date</b>", row=4, col=1)
            
            return fig
            
        except Exception as e:
            self.logger.error("‚ùå Time series decomposition chart creation failed", e)
            return self.create_error_figure("Failed to create time series decomposition")
    
    def create_anomaly_detection_plot(self, df: pd.DataFrame) -> go.Figure:
        """
        NEW PLOT 5: Anomaly Detection - Highlight anomalous days in mail/call relationship
        """
        if df.empty or self.dp.anomaly_data.empty:
            return self.create_error_figure("Anomaly detection data not available")
        
        try:
            # Create main time series with anomalies highlighted
            fig = go.Figure()
            
            # Add normal data points
            fig.add_trace(
                go.Scatter(
                    x=df['date'],
                    y=df['call_volume'],
                    mode='lines+markers',
                    name='Call Volume',
                    line={'color': self.theme.primary_blue, 'width': 3},
                    marker={'size': 6, 'color': self.theme.primary_blue},
                    hovertemplate='<b>Call Volume</b><br>Date: %{x}<br>Volume: %{y:,.0f}<extra></extra>'
                )
            )
            
            # Add mail volume on secondary y-axis
            fig.add_trace(
                go.Scatter(
                    x=df['date'],
                    y=df['mail_volume'],
                    mode='lines',
                    name='Mail Volume',
                    line={'color': self.theme.primary_teal, 'width': 3, 'dash': 'dot'},
                    yaxis='y2',
                    hovertemplate='<b>Mail Volume</b><br>Date: %{x}<br>Volume: %{y:,.0f}<extra></extra>'
                )
            )
            
            # Highlight anomalies
            anomaly_data = self.dp.anomaly_data
            
            if not anomaly_data.empty:
                fig.add_trace(
                    go.Scatter(
                        x=anomaly_data['date'],
                        y=anomaly_data['call_volume'],
                        mode='markers',
                        name='Anomalies',
                        marker={
                            'size': 15,
                            'color': self.theme.error,
                            'symbol': 'diamond',
                            'line': {'width': 2, 'color': 'white'}
                        },
                        hovertemplate='<b>ANOMALY</b><br>Date: %{x}<br>Call Volume: %{y:,.0f}<br>Reason: %{text}<extra></extra>',
                        text=anomaly_data['anomaly_reason']
                    )
                )
                
                # Add anomaly annotations for top 5 anomalies
                top_anomalies = anomaly_data.nsmallest(5, 'anomaly_score')
                
                for _, anomaly in top_anomalies.iterrows():
                    fig.add_annotation(
                        x=anomaly['date'],
                        y=anomaly['call_volume'],
                        text=f"‚ö†Ô∏è {anomaly['anomaly_reason'][:30]}...",
                        showarrow=True,
                        arrowhead=2,
                        arrowsize=1,
                        arrowwidth=2,
                        arrowcolor=self.theme.error,
                        bgcolor=self.theme.neutral_50,
                        bordercolor=self.theme.error,
                        borderwidth=1,
                        font={'size': 12}
                    )
            
            # Add control limits (mean ¬± 2 std dev)
            call_mean = df['call_volume'].mean()
            call_std = df['call_volume'].std()
            
            fig.add_hline(
                y=call_mean + 2 * call_std,
                line_dash="dash",
                line_color=self.theme.warning,
                annotation_text="Upper Control Limit",
                annotation_position="bottom right"
            )
            
            fig.add_hline(
                y=call_mean - 2 * call_std,
                line_dash="dash",
                line_color=self.theme.warning,
                annotation_text="Lower Control Limit",
                annotation_position="top right"
            )
            
            fig.add_hline(
                y=call_mean,
                line_dash="solid",
                line_color=self.theme.neutral_400,
                annotation_text="Mean",
                annotation_position="top left"
            )
            
            # Update layout with dual y-axes
            fig.update_layout(
                title={
                    'text': '<b>üö® Anomaly Detection Analysis</b><br><sub>Identifying unusual patterns in mail-call relationships</sub>',
                    'font': {'size': 24, 'color': self.theme.neutral_900},
                    'x': 0.5
                },
                xaxis_title="<b>Date</b>",
                yaxis=dict(
                    title="<b>Call Volume</b>",
                    titlefont={'color': self.theme.primary_blue}
                ),
                yaxis2=dict(
                    title="<b>Mail Volume</b>",
                    titlefont={'color': self.theme.primary_teal},
                    overlaying='y',
                    side='right'
                ),
                height=self.config['CHART_HEIGHT'],
                **self.base_layout
            )
            
            return fig
            
        except Exception as e:
            self.logger.error("‚ùå Anomaly detection plot creation failed", e)
            return self.create_error_figure("Failed to create anomaly detection plot")

# =============================================================================
# MAIN EXECUTION FUNCTION
# =============================================================================


# =============================================================================
# ENTERPRISE DASHBOARD APPLICATION v3.0
# =============================================================================

class EnterpriseDashboardApp:
    """
    Enterprise-grade dashboard application with premium UX/UI
    """
    
    def __init__(self, data_processor: EnterpriseDataProcessor):
        self.dp = data_processor
        self.logger = data_processor.logger
        self.theme = THEME
        self.config = ENTERPRISE_CONFIG
        
        if not DASH_AVAILABLE:
            self.logger.error("‚ùå Dash framework not available - cannot create dashboard")
            raise ImportError("Dash framework required for dashboard")
        
        # Initialize Dash app with premium theme
        self.app = dash.Dash(
            __name__,
            external_stylesheets=[dbc.themes.BOOTSTRAP],
        )
        
        self.app.title = "Customer Communications Intelligence v3.0"
        
        # Initialize visualization suite
        self.viz = PremiumDashboardVisualizations(data_processor)
        
        # Setup dashboard
        self.setup_layout()
        self.setup_callbacks()
        
        self.logger.info("‚úÖ Enterprise Dashboard Application v3.0 initialized")
    
    def setup_layout(self):
        """Setup basic dashboard layout"""
        self.app.layout = html.Div([
            html.H1("Customer Communications Intelligence v3.0"),
            html.P("Dashboard loading..."),
            dcc.Graph(id='main-chart', figure=self.viz.create_premium_overview_chart(self.dp.combined_df))
        ])
    
    def setup_callbacks(self):
        """Setup basic callbacks"""
        pass
    
    def run(self, debug: bool = False, port: int = 8050, host: str = '127.0.0.1'):
        """Run the dashboard"""
        try:
            self.logger.info(f"üöÄ Starting Enterprise Dashboard v3.0 at http://{host}:{port}")
            self.app.run(debug=debug, port=port, host=host)
        except Exception as e:
            self.logger.error("‚ùå Dashboard failed to start", e)
            raise










def main():
    """
    Main execution function for Enterprise Dashboard v3.0
    """
    print("üî• CUSTOMER COMMUNICATIONS INTELLIGENCE DASHBOARD v3.0")
    print("=" * 80)
    print("üéØ ENTERPRISE FEATURES:")
    print("   ‚Ä¢ Dual call data source management (volume + intents)")
    print("   ‚Ä¢ Mail data filtered to call data presence")
    print("   ‚Ä¢ 4 NEW advanced plots: Lag Analysis, Scatter Trends, Time Series Decomposition, Anomaly Detection")
    print("   ‚Ä¢ Premium UX/UI with enhanced styling and interactivity")
    print("   ‚Ä¢ Production-grade error handling and logging")
    print("   ‚Ä¢ Enhanced correlation analysis with multiple feature types")
    print("   ‚Ä¢ Financial data normalization and proper scaling")
    print("   ‚Ä¢ Top 10 + Other grouping for cleaner visualizations")
    print("=" * 80)
    
    # Initialize enterprise logger
    logger = EnterpriseLogger("CustomerCommsDashboard", ENTERPRISE_CONFIG['LOG_LEVEL'])
    
    try:
        # Dependency check
        if not DASH_AVAILABLE or not PLOTLY_AVAILABLE:
            logger.critical("‚ùå Required dependencies missing")
            print("‚ùå DEPENDENCY ERROR: Missing required packages")
            print("üì¶ Install with: pip install plotly dash dash-bootstrap-components")
            if not SCIPY_AVAILABLE:
                print("üì¶ For advanced analytics: pip install scipy")
            if not SKLEARN_AVAILABLE:
                print("üì¶ For ML features: pip install scikit-learn")
            if not STATSMODELS_AVAILABLE:
                print("üì¶ For time series: pip install statsmodels")
            return False
        
        logger.info("üèóÔ∏è Initializing Enterprise Data Processor v3.0...")
        
        # Initialize data processor
        dp = EnterpriseDataProcessor(ENTERPRISE_CONFIG, logger)
        
        # Execute master data pipeline
        logger.info("üì• Phase 1: Data Loading")
        call_volume_success = dp.load_call_volume_data()
        call_intent_success = dp.load_call_intent_data()
        mail_success = dp.load_mail_data()
        financial_success = dp.load_financial_data()

        # Check if we have minimum required data
        if not call_volume_success and not call_intent_success:
            logger.warning("‚ö†Ô∏è No call data available, creating sample data...")
            if not dp.create_sample_data():
                logger.error("‚ùå Failed to create sample data")
                return False

        # Phase 2: Data Combination and Processing
        logger.info("‚öôÔ∏è Phase 2: Data Combination and Processing")
        pipeline_success = dp.combine_and_process_data()

        if not pipeline_success:
            logger.error("‚ùå Data combination failed")
            return False

        # Phase 3: Advanced Analytics
        logger.info("üî¨ Phase 3: Advanced Analytics")
        dp.analyze_enhanced_correlations()
        dp.analyze_rolling_correlation()
        dp.analyze_intent_correlation()
        dp.calculate_efficiency_metrics()

        # Phase 4: Advanced Features
        logger.info("üéØ Phase 4: Advanced Features")
        dp.detect_anomalies()
        dp.perform_time_series_decomposition()

        logger.info("‚úÖ All processing phases completed successfully")
                
        if not pipeline_success:
            logger.error("‚ùå Master data pipeline failed")
            return False
        
        # Initialize dashboard application
        logger.info("üé® Initializing Premium Dashboard Application...")
        dashboard = EnterpriseDashboardApp(dp)
        
        # Generate comprehensive status report
        logger.info("=" * 80)
        logger.info("üìä ENTERPRISE DASHBOARD v3.0 STATUS REPORT")
        logger.info("=" * 80)
        
        # Data status
        logger.info("üì• DATA SOURCES:")
        logger.info(f"   ‚Ä¢ Call Volume Data: {'‚úÖ LOADED' if dp.has_call_volume_data else '‚ùå MISSING'} ({len(dp.call_volume_df):,} records)")
        logger.info(f"   ‚Ä¢ Call Intent Data: {'‚úÖ LOADED' if dp.has_call_intent_data else '‚ùå MISSING'} ({len(dp.call_intent_df):,} records)")
        logger.info(f"   ‚Ä¢ Mail Data: {'‚úÖ LOADED' if dp.has_mail_data else '‚ùå MISSING'} ({len(dp.mail_df):,} records)")
        logger.info(f"   ‚Ä¢ Financial Data: {'‚úÖ LOADED' if dp.has_financial_data else '‚ùå MISSING'} ({len(dp.financial_df):,} records)")
        
        # Processing status
        logger.info("‚öôÔ∏è DATA PROCESSING:")
        logger.info(f"   ‚Ä¢ Combined Dataset: {len(dp.combined_df):,} business day records")
        logger.info(f"   ‚Ä¢ Date Range: {dp.combined_df['date'].min().date()} to {dp.combined_df['date'].max().date()}")
        logger.info(f"   ‚Ä¢ Weekends Removed: {dp.data_quality_metrics.get('weekends_removed', 0):,}")
        logger.info(f"   ‚Ä¢ Holidays Removed: {dp.data_quality_metrics.get('holidays_removed', 0):,}")
        logger.info(f"   ‚Ä¢ Outliers Removed: {dp.data_quality_metrics.get('outliers_removed', 0):,}")
        
        # Analytics status
        logger.info("üî¨ ANALYTICS RESULTS:")
        logger.info(f"   ‚Ä¢ Momentum Correlations: {len(dp.momentum_correlation_results):,} calculated")
        logger.info(f"   ‚Ä¢ Rolling Correlations: {len(dp.rolling_correlation_results):,} windows analyzed")
        logger.info(f"   ‚Ä¢ Intent Correlations: {len(dp.intent_correlation_results):,} intents analyzed")
        logger.info(f"   ‚Ä¢ Anomalies Detected: {len(dp.anomaly_data):,} anomalous patterns")
        logger.info(f"   ‚Ä¢ Time Series Decomposition: {'‚úÖ COMPLETED' if dp.decomposition_results else '‚ùå FAILED'}")
        
        # Key insights
        if not dp.momentum_correlation_results.empty:
            best_corr = dp.momentum_correlation_results.iloc[0]
            logger.info("üéØ KEY INSIGHTS:")
            logger.info(f"   ‚Ä¢ Best Correlation: {best_corr['correlation']:.3f} ({best_corr['feature']})")
            logger.info(f"   ‚Ä¢ Optimal Lag: {best_corr['lag_days']} days")
            logger.info(f"   ‚Ä¢ Feature Category: {best_corr['category']}")
            
            # Top 3 correlations
            logger.info("   ‚Ä¢ Top 3 Correlations:")
            for i, (_, row) in enumerate(dp.momentum_correlation_results.head(3).iterrows()):
                logger.info(f"     {i+1}. {row['feature']} ({row['lag_days']}d): {row['correlation']:.3f}")
        
        # Performance metrics
        if dp.efficiency_metrics:
            logger.info("üìà PERFORMANCE METRICS:")
            logger.info(f"   ‚Ä¢ Response Rate: {dp.efficiency_metrics['response_rate_pct']:.2f}%")
            logger.info(f"   ‚Ä¢ Average Daily Calls: {dp.efficiency_metrics['avg_daily_calls']:.0f}")
            logger.info(f"   ‚Ä¢ Average Daily Mail: {dp.efficiency_metrics['avg_daily_mail']:.0f}")
            logger.info(f"   ‚Ä¢ Data Quality Score: {dp.efficiency_metrics['data_quality_score']:.0f}%")
        
        # Feature status
        logger.info("üéØ FEATURE STATUS:")
        logger.info(f"   ‚Ä¢ Lag Analysis Heatmap: {'‚úÖ AVAILABLE' if dp.has_mail_data else '‚ùå REQUIRES MAIL DATA'}")
        logger.info(f"   ‚Ä¢ Scatter Trend Analysis: {'‚úÖ AVAILABLE' if len(dp.combined_df) > 10 else '‚ùå INSUFFICIENT DATA'}")
        logger.info(f"   ‚Ä¢ Time Series Decomposition: {'‚úÖ AVAILABLE' if dp.decomposition_results else '‚ùå REQUIRES MORE DATA'}")
        logger.info(f"   ‚Ä¢ Anomaly Detection: {'‚úÖ AVAILABLE' if not dp.anomaly_data.empty else '‚ùå NO ANOMALIES DETECTED'}")
        logger.info(f"   ‚Ä¢ Intent Analysis: {'‚úÖ AVAILABLE' if dp.has_call_intent_data else '‚ùå REQUIRES INTENT DATA'}")
        
        logger.info("=" * 80)
        logger.info("üåê DASHBOARD ACCESS:")
        logger.info("   ‚Ä¢ URL: http://127.0.0.1:8050")
        logger.info("   ‚Ä¢ Status: Ready for executive presentation")
        logger.info("üõë Press Ctrl+C to stop the dashboard")
        logger.info("=" * 80)
        
        # Launch dashboard
        dashboard.run(
            debug=ENTERPRISE_CONFIG['DEBUG_MODE'],
            port=8050,
            host='127.0.0.1'
        )
        
        return True
        
    except KeyboardInterrupt:
        logger.info("üõë Dashboard stopped by user")
        return True
        
    except Exception as e:
        logger.critical("‚ùå Critical system failure", e)
        return False

if __name__ == '__main__':
    # Pre-flight system check
    print("üîç SYSTEM PRE-FLIGHT CHECK:")
    print("=" * 50)
    
    dependencies = {
        'Plotly': PLOTLY_AVAILABLE,
        'Dash': DASH_AVAILABLE,
        'SciPy': SCIPY_AVAILABLE,
        'Scikit-learn': SKLEARN_AVAILABLE,
        'Statsmodels': STATSMODELS_AVAILABLE,
        'yfinance': FINANCIAL_AVAILABLE
    }
    
    for dep, available in dependencies.items():
        status = "‚úÖ AVAILABLE" if available else "‚ùå MISSING"
        print(f"   {dep:<15}: {status}")
    
    print("=" * 50)
    
    # Check critical dependencies
    critical_missing = []
    if not PLOTLY_AVAILABLE:
        critical_missing.append('plotly')
    if not DASH_AVAILABLE:
        critical_missing.append('dash dash-bootstrap-components')
    
    if critical_missing:
        print("‚ùå CRITICAL DEPENDENCIES MISSING!")
        print("üì¶ Install with:")
        for dep in critical_missing:
            print(f"   pip install {dep}")
        print("\nüîß Optional but recommended:")
        if not SCIPY_AVAILABLE:
            print("   pip install scipy")
        if not SKLEARN_AVAILABLE:
            print("   pip install scikit-learn")
        if not STATSMODELS_AVAILABLE:
            print("   pip install statsmodels")
        if not FINANCIAL_AVAILABLE:
            print("   pip install yfinance")
        print("\nüö´ Cannot start dashboard without critical dependencies.")
        sys.exit(1)
    
    print("‚úÖ All critical dependencies available!")
    print("üöÄ Starting Enterprise Dashboard v3.0...\n")
    
    # Execute main function
    success = main()
    
    if success:
        print("\n‚úÖ Enterprise Dashboard v3.0 session completed successfully")
        print("üéØ Thank you for using Customer Communications Intelligence Platform")
    else:
        print("\n‚ö†Ô∏è Dashboard session ended with errors")
        sys.exit(1)
    
    sys.exit(0)

# =============================================================================
# END OF ENHANCED CUSTOMER COMMUNICATIONS DASHBOARD v3.0
# =============================================================================
